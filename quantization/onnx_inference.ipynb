{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def load_model(onnx_file_path):\n",
    "    session = ort.InferenceSession(onnx_file_path, providers=['CPUExecutionProvider'])\n",
    "    return session\n",
    "\n",
    "def run_model(session, input_data):\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_names = [output.name for output in session.get_outputs()]\n",
    "    print(\"22\", len(output_names))\n",
    "    print(input_name)\n",
    "    results = session.run(output_names, {input_name: input_data})\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Works for Quantized Int8 and Unquantized model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess_image(x, mode='caffe'):\n",
    "    \n",
    "    x = x.astype(np.float32)\n",
    "\n",
    "    if mode == 'tf':\n",
    "        x /= 127.5\n",
    "        x -= 1.\n",
    "    elif mode == 'caffe':\n",
    "        x -= [103.939, 116.779, 123.68]\n",
    "\n",
    "    return x\n",
    "\n",
    "def resize_image(img, min_side=900, max_side=1200):\n",
    "\n",
    "    scale = compute_resize_scale(img.shape, min_side=min_side, max_side=max_side)\n",
    "\n",
    "    # resize the image with the computed scale\n",
    "    img = cv2.resize(img, None, fx=scale, fy=scale)\n",
    "\n",
    "    return img, scale\n",
    "\n",
    "def compute_resize_scale(image_shape, min_side=900, max_side=1200):\n",
    "    \n",
    "    (rows, cols, _) = image_shape\n",
    "\n",
    "    smallest_side = min(rows, cols)\n",
    "\n",
    "    # rescale the image so the smallest side is min_side\n",
    "    scale = min_side / smallest_side\n",
    "\n",
    "    # check if the largest side is now greater than max_side, which can happen\n",
    "    # when images have a large aspect ratio\n",
    "    largest_side = max(rows, cols)\n",
    "    if largest_side * scale > max_side:\n",
    "        scale = max_side / largest_side\n",
    "\n",
    "    return scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 1200, 3)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path_ = './images/validation_image_cmu/10.6.3.9 Flat margin.jpg'\n",
    "\n",
    "image = Image.open(image_path_).convert('RGB')\n",
    "\n",
    "image = image.rotate(90)\n",
    "image = np.asarray(image)\n",
    "image = image[:, :, ::-1].copy()\n",
    "image = preprocess_image(image)\n",
    "image__, scale = resize_image(image)\n",
    "image__.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_(session, input_data):\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_names = [output.name for output in session.get_outputs()]\n",
    "    print(\"22\", len(output_names))\n",
    "    print(input_name)\n",
    "    results = session.run(output_names, {input_name: input_data})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unquantized Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_file_path = \"path_to.onnx\"\n",
    "session_1 = ort.InferenceSession(onnx_file_path, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "results = run_model_(session_1, np.expand_dims(image__, axis=0))\n",
    "end_time = time.time()\n",
    "\n",
    "print('Inference session.get_outputs():',results)\n",
    "print('Inference Time:', end_time - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantized Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_file_path_quant = \"path_to.onnx\"\n",
    "session_quant = ort.InferenceSession(onnx_file_path_quant, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "results = run_model_(session_quant, np.expand_dims(image__, axis=0))\n",
    "end_time = time.time()\n",
    "\n",
    "print('Inference session.get_outputs():',results)\n",
    "print('Inference Time:', end_time - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FP16 Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_file_path_fp16 = \".path_to_FP16.onnx\"\n",
    "session_fp16 = ort.InferenceSession(onnx_file_path_fp16, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 3\n",
      "input_1:0\n",
      "Inference session.get_outputs(): [array([[[ 1.830e+02,  1.672e+01,  1.094e+03,  9.000e+02],\n",
      "        [ 2.141e+02,  0.000e+00,  1.105e+03,  9.000e+02],\n",
      "        [-1.000e+00, -1.000e+00, -1.000e+00, -1.000e+00],\n",
      "        ...,\n",
      "        [-1.000e+00, -1.000e+00, -1.000e+00, -1.000e+00],\n",
      "        [-1.000e+00, -1.000e+00, -1.000e+00, -1.000e+00],\n",
      "        [-1.000e+00, -1.000e+00, -1.000e+00, -1.000e+00]]], dtype=float16), array([[ 0.541 ,  0.2477, -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ,\n",
      "        -1.    , -1.    , -1.    , -1.    , -1.    , -1.    ]],\n",
      "      dtype=float16), array([[ 1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]], dtype=int32)]\n",
      "Inference Time: 5.951932191848755 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "results = run_model_(session_fp16, np.expand_dims(image__.astype(np.float16), axis=0))\n",
    "end_time = time.time()\n",
    "\n",
    "print('Inference session.get_outputs():',results)\n",
    "print('Inference Time:', end_time - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_pc_cpu(model_path, image_paths):\n",
    "    \n",
    "    labels_to_names = {0: 'non_cancer', 1: 'cancer'}\n",
    "    model = load_model(model_path, backbone_name='resnet50')\n",
    "    try:\n",
    "        model = models.convert_model(model)\n",
    "    except:\n",
    "        print(\"Model is likely already an inference model\")\n",
    "    results = []\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        # Load and preprocess the image\n",
    "        # print(\"Loading Image: {}\".format(image_path))\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        image = image.rotate(90)\n",
    "            \n",
    "        image = np.asarray(image)\n",
    "        image = image[:, :, ::-1].copy()\n",
    "        image = preprocess_image(image)\n",
    "        image, scale = resize_image(image)\n",
    "\n",
    "        # Run the inference\n",
    "        start = time.time()\n",
    "        boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "        inference_time = time.time() - start\n",
    "        predicted_label = labels[0][np.argmax(scores[0])]\n",
    "        confidence = scores[0][np.argmax(scores[0])]\n",
    "        \n",
    "        results.append({'isCancerDetected': predicted_label, 'InferenceTime': inference_time, 'Confidence2': confidence})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantization to INT8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = './assets/resnet50_csv_26_inf_900x1200.onnx'\n",
    "model_quant = './assets/resnet50_csv_26_inf_900x1200_Quantized.onnx'\n",
    "\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantization to FP16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxconverter_common import float16\n",
    "\n",
    "model = onnx.load(\"./assets/resnet50_csv_26_inf_900x1200.onnx\")\n",
    "model_fp16 = float16.convert_float_to_float16(model)\n",
    "onnx.save(model_fp16, \"./assets/resnet50_csv_26_inf_900x1200_FP16.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 9.5367431640625e-07 seconds\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "def load_model(onnx_file_path):\n",
    "    session = ort.InferenceSession(onnx_file_path, providers=['CPUExecutionProvider'])\n",
    "    return session\n",
    "\n",
    "def run_model(session, input_data):\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_names = [output.name for output in session.get_outputs()]\n",
    "    print(\"22\", len(output_names))\n",
    "    print(input_name)\n",
    "    results = session.run(output_names, {input_name: input_data})\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    \n",
    "    start_time = time.time()\n",
    "    # results = run_model(session, image_array)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # print('Inference session.get_outputs():',results)\n",
    "    print('Inference Time:', end_time - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
