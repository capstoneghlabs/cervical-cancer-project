{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_retinanet.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras = load_model('snapshots/resnet50_csv_50.h5', backbone_name='resnet50')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# tflite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "For full integer quantization, a `representative_dataset` must be specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m converter\u001b[39m.\u001b[39moptimizations \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mlite\u001b[39m.\u001b[39mOptimize\u001b[39m.\u001b[39mDEFAULT]\n\u001b[1;32m      2\u001b[0m converter\u001b[39m.\u001b[39mtarget_spec\u001b[39m.\u001b[39msupported_types \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mint8]\n\u001b[0;32m----> 3\u001b[0m tflite_quant_model \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39;49mconvert()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1065\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(convert_func)\n\u001b[1;32m   1063\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1064\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1065\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_and_export_metrics(convert_func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1040\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[39m\"\"\"Wraps around convert function to export metrics.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \n\u001b[1;32m   1031\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[39m  The decorator to wrap the convert function.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_increase_conversion_attempt_metric()\n\u001b[0;32m-> 1040\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_conversion_params_metric()\n\u001b[1;32m   1041\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[1;32m   1042\u001b[0m result \u001b[39m=\u001b[39m convert_func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:875\u001b[0m, in \u001b[0;36mTFLiteConverterBase._save_conversion_params_metric\u001b[0;34m(self, graph_def, inference_type, inference_input_type)\u001b[0m\n\u001b[1;32m    872\u001b[0m converter_kwargs\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_base_converter_args())\n\u001b[1;32m    874\u001b[0m \u001b[39m# Optimization parameters.\u001b[39;00m\n\u001b[0;32m--> 875\u001b[0m quant_mode \u001b[39m=\u001b[39m QuantizationMode(\n\u001b[1;32m    876\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizations,\n\u001b[1;32m    877\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_spec,\n\u001b[1;32m    878\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepresentative_dataset,\n\u001b[1;32m    879\u001b[0m     graph_def,\n\u001b[1;32m    880\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_experimental_disable_per_channel,\n\u001b[1;32m    881\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexperimental_new_dynamic_range_quantizer,\n\u001b[1;32m    882\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_experimental_low_bit_qat,\n\u001b[1;32m    883\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_experimental_full_integer_quantization_bias_type,\n\u001b[1;32m    884\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_experimental_variable_quantization,\n\u001b[1;32m    885\u001b[0m )\n\u001b[1;32m    886\u001b[0m converter_kwargs\u001b[39m.\u001b[39mupdate({\n\u001b[1;32m    887\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtf_version\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\u001b[39m.\u001b[39menvironment\u001b[39m.\u001b[39mtensorflowVersion,\n\u001b[1;32m    888\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mapi_version\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\u001b[39m.\u001b[39menvironment\u001b[39m.\u001b[39mapiVersion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mactivations_type\u001b[39m\u001b[39m\"\u001b[39m: quant_mode\u001b[39m.\u001b[39mactivations_type(),\n\u001b[1;32m    906\u001b[0m })\n\u001b[1;32m    907\u001b[0m converter_kwargs\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m    908\u001b[0m     quant_mode\u001b[39m.\u001b[39mconverter_flags(inference_type, inference_input_type)\n\u001b[1;32m    909\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:267\u001b[0m, in \u001b[0;36mQuantizationMode.__init__\u001b[0;34m(self, optimizations, target_spec, representative_dataset, graph_def, disable_per_channel, experimental_new_dynamic_range_quantizer, experimental_low_bit_qat, full_integer_quantization_bias_type, experimental_mlir_variable_quantization)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_representative_dataset \u001b[39m=\u001b[39m representative_dataset\n\u001b[1;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_def \u001b[39m=\u001b[39m graph_def\n\u001b[0;32m--> 267\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_int8_required()\n\u001b[1;32m    268\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_per_channel \u001b[39m=\u001b[39m disable_per_channel\n\u001b[1;32m    270\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_new_dynamic_range_quantizer \u001b[39m=\u001b[39m (\n\u001b[1;32m    271\u001b[0m     experimental_new_dynamic_range_quantizer\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:498\u001b[0m, in \u001b[0;36mQuantizationMode._validate_int8_required\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[39m# Check if representative_dataset is specified.\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    495\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_representative_dataset\n\u001b[1;32m    496\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_quantization_aware_training()\n\u001b[1;32m    497\u001b[0m ):\n\u001b[0;32m--> 498\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    499\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mFor full integer quantization, a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m`representative_dataset` must be specified.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    501\u001b[0m   )\n\u001b[1;32m    503\u001b[0m \u001b[39m# Update represenative dataset to the expected format.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_representative_dataset:\n",
      "\u001b[0;31mValueError\u001b[0m: For full integer quantization, a `representative_dataset` must be specified."
     ]
    }
   ],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.int8]\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.tflitef16', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFITE 16 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 18:35:48.250271: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-20 18:35:48.250337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-20 18:35:48.301177: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-20 18:35:48.415054: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-20 18:35:49.814558: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m converter \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mlite\u001b[39m.\u001b[39mTFLiteConverter\u001b[39m.\u001b[39mfrom_keras_model(model)\n\u001b[1;32m      2\u001b[0m converter\u001b[39m.\u001b[39moptimizations \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mlite\u001b[39m.\u001b[39mOptimize\u001b[39m.\u001b[39mDEFAULT]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Physical devices cannot be modified after being initialized\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# show images inline\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules when they have changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# import keras_retinanet\n",
    "from keras_retinanet import models\n",
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "from keras_retinanet.utils.colors import label_color\n",
    "from keras_retinanet.utils.gpu import setup_gpu\n",
    "\n",
    "# import miscellaneous modules\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# use this to change which GPU to use\n",
    "gpu = '0'\n",
    "\n",
    "# set the modified tf session as backend in keras\n",
    "setup_gpu(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# configuration parameters \n",
    "TEST_DATA_DIR = 'PyAVE_cmu_fall2023/cmu_fall2023/images/validation_image_cmu/'\n",
    "MODEL_PATH = \"GHL_Models/resnet50_csv_26.h5\"\n",
    "TFLITE_MODEL_DIR = \"./models/tflite/\"\n",
    "TEST_SAMPLES = 1\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory to save the tflite models if it does not exists\n",
    "if not os.path.exists(TFLITE_MODEL_DIR):\n",
    "    os.makedirs(TFLITE_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('GHL_Models/resnet50_csv_26.h5', backbone_name='resnet50')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "from keras_retinanet.preprocessing.csv_generator import CSVGenerator\n",
    "\n",
    "\n",
    "\n",
    "def create_generator():\n",
    "    \"\"\" Create generators for evaluation.\n",
    "    \"\"\"\n",
    "    common_args = {\n",
    "        \n",
    "        'preprocess_image' : preprocess_image,\n",
    "        \n",
    "    }\n",
    "\n",
    "    \n",
    "    validation_generator = CSVGenerator(\n",
    "           'val_anots.csv',\n",
    "            'class.csv',\n",
    "            shuffle_groups=False,\n",
    "            **common_args\n",
    "        )\n",
    "    \n",
    "    return validation_generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = create_generator()\n",
    "# create an image generator with a batch size of 1 \n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "test_generator = gen\n",
    "def represent_data_gen():\n",
    "    \"\"\" it yields an image one by one \"\"\"\n",
    "    for ind in range(len(gen.image_data.keys())):\n",
    "        img = test_generator.load_image(ind) # it returns (image and label) tuple\n",
    "        img, scale = resize_image(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        print(img.shape)\n",
    "        yield [np.array(img, dtype=np.float32, ndmin=2)] # return only image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpo8uygoft/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpo8uygoft/assets\n",
      "/home/developer/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2023-11-20 18:46:22.858896: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-11-20 18:46:22.859430: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-11-20 18:46:22.860514: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpo8uygoft\n",
      "2023-11-20 18:46:22.895341: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-11-20 18:46:22.895372: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpo8uygoft\n",
      "2023-11-20 18:46:22.992613: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-11-20 18:46:24.108098: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpo8uygoft\n",
      "2023-11-20 18:46:24.396130: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 1535616 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 150, Total Ops 357, % non-converted = 42.02 %\n",
      " * 150 ARITH ops\n",
      "\n",
      "- arith.constant:  150 occurrences  (f32: 142, i32: 8)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 18)\n",
      "  (f32: 2)\n",
      "  (f32: 111)\n",
      "  (f32: 5)\n",
      "  (f32: 1)\n",
      "  (i32: 12)\n",
      "  (f32: 16)\n",
      "  (f32: 1)\n",
      "  (f32: 10)\n",
      "  (f32: 2)\n",
      "  (i32: 12)\n",
      "  (i32: 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 800, 1067, 3)\n",
      "(1, 1201, 800, 3)\n",
      "(1, 800, 1015, 3)\n",
      "(1, 800, 1067, 3)\n",
      "(1, 800, 926, 3)\n",
      "(1, 800, 1201, 3)\n",
      "(1, 800, 1102, 3)\n",
      "(1, 800, 1201, 3)\n",
      "(1, 1067, 800, 3)\n",
      "(1, 800, 1000, 3)\n",
      "(1, 800, 1061, 3)\n",
      "(1, 1205, 800, 3)\n",
      "(1, 800, 1201, 3)\n",
      "(1, 800, 980, 3)\n",
      "(1, 800, 1067, 3)\n",
      "(1, 800, 1294, 3)\n",
      "(1, 1067, 800, 3)\n",
      "(1, 1067, 800, 3)\n",
      "(1, 800, 1250, 3)\n",
      "(1, 800, 1067, 3)\n",
      "(1, 800, 1194, 3)\n",
      "(1, 800, 1067, 3)\n",
      "(1, 800, 1067, 3)\n",
      "(1, 800, 1067, 3)\n",
      "(1, 800, 877, 3)\n",
      "(1, 800, 1170, 3)\n",
      "(1, 800, 1067, 3)\n",
      "(1, 1120, 800, 3)\n",
      "(1, 1201, 800, 3)\n",
      "(1, 800, 808, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
     ]
    }
   ],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = represent_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37381296"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_quant_file = tflite_models_dir/\"/home/developer/Documents/CMU/Capstone_Codes/cervical-cancer-project/quantized_models/full_int8.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpqkj5mc66/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpqkj5mc66/assets\n",
      "2023-11-20 19:31:28.282939: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-11-20 19:31:28.282961: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-11-20 19:31:28.283136: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpqkj5mc66\n",
      "2023-11-20 19:31:28.308058: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-11-20 19:31:28.308099: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpqkj5mc66\n",
      "2023-11-20 19:31:28.384684: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-11-20 19:31:29.449414: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpqkj5mc66\n",
      "2023-11-20 19:31:29.732068: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 1448932 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 150, Total Ops 357, % non-converted = 42.02 %\n",
      " * 150 ARITH ops\n",
      "\n",
      "- arith.constant:  150 occurrences  (f32: 142, i32: 8)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 18)\n",
      "  (f32: 2)\n",
      "  (f32: 111)\n",
      "  (f32: 5)\n",
      "  (f32: 1)\n",
      "  (i32: 12)\n",
      "  (f32: 16)\n",
      "  (f32: 1)\n",
      "  (f32: 10)\n",
      "  (f32: 2)\n",
      "  (i32: 12)\n",
      "  (i32: 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TFLite ModelAnalyzer ===\n",
      "\n",
      "Your TFLite model has '1' subgraph(s). In the subgraph description below,\n",
      "T# represents the Tensor numbers. For example, in Subgraph#0, the CONV_2D op takes\n",
      "tensor #0 and tensor #72 and tensor #1 as input and produces tensor #151 as output.\n",
      "\n",
      "Subgraph#0 main(T#0) -> [T#354, T#344]\n",
      "  Op#0 CONV_2D(T#0, T#72, T#1) -> [T#151]\n",
      "  Op#1 MAX_POOL_2D(T#151) -> [T#152]\n",
      "  Op#2 CONV_2D(T#152, T#73, T#42) -> [T#153]\n",
      "  Op#3 CONV_2D(T#152, T#74, T#2) -> [T#154]\n",
      "  Op#4 PAD(T#154, T#149[0, 0, 1, 1, 1, ...]) -> [T#155]\n",
      "  Op#5 CONV_2D(T#155, T#75, T#3) -> [T#156]\n",
      "  Op#6 CONV_2D(T#156, T#76, T#43) -> [T#157]\n",
      "  Op#7 ADD(T#157, T#153) -> [T#158]\n",
      "  Op#8 CONV_2D(T#158, T#77, T#4) -> [T#159]\n",
      "  Op#9 PAD(T#159, T#149[0, 0, 1, 1, 1, ...]) -> [T#160]\n",
      "  Op#10 CONV_2D(T#160, T#78, T#5) -> [T#161]\n",
      "  Op#11 CONV_2D(T#161, T#79, T#44) -> [T#162]\n",
      "  Op#12 ADD(T#162, T#158) -> [T#163]\n",
      "  Op#13 CONV_2D(T#163, T#80, T#6) -> [T#164]\n",
      "  Op#14 PAD(T#164, T#149[0, 0, 1, 1, 1, ...]) -> [T#165]\n",
      "  Op#15 CONV_2D(T#165, T#81, T#7) -> [T#166]\n",
      "  Op#16 CONV_2D(T#166, T#82, T#45) -> [T#167]\n",
      "  Op#17 ADD(T#167, T#163) -> [T#168]\n",
      "  Op#18 CONV_2D(T#168, T#83, T#46) -> [T#169]\n",
      "  Op#19 CONV_2D(T#168, T#84, T#8) -> [T#170]\n",
      "  Op#20 PAD(T#170, T#149[0, 0, 1, 1, 1, ...]) -> [T#171]\n",
      "  Op#21 CONV_2D(T#171, T#85, T#9) -> [T#172]\n",
      "  Op#22 CONV_2D(T#172, T#86, T#47) -> [T#173]\n",
      "  Op#23 ADD(T#173, T#169) -> [T#174]\n",
      "  Op#24 CONV_2D(T#174, T#87, T#10) -> [T#175]\n",
      "  Op#25 PAD(T#175, T#149[0, 0, 1, 1, 1, ...]) -> [T#176]\n",
      "  Op#26 CONV_2D(T#176, T#88, T#11) -> [T#177]\n",
      "  Op#27 CONV_2D(T#177, T#89, T#48) -> [T#178]\n",
      "  Op#28 ADD(T#178, T#174) -> [T#179]\n",
      "  Op#29 CONV_2D(T#179, T#90, T#12) -> [T#180]\n",
      "  Op#30 PAD(T#180, T#149[0, 0, 1, 1, 1, ...]) -> [T#181]\n",
      "  Op#31 CONV_2D(T#181, T#91, T#13) -> [T#182]\n",
      "  Op#32 CONV_2D(T#182, T#92, T#49) -> [T#183]\n",
      "  Op#33 ADD(T#183, T#179) -> [T#184]\n",
      "  Op#34 CONV_2D(T#184, T#93, T#14) -> [T#185]\n",
      "  Op#35 PAD(T#185, T#149[0, 0, 1, 1, 1, ...]) -> [T#186]\n",
      "  Op#36 CONV_2D(T#186, T#94, T#15) -> [T#187]\n",
      "  Op#37 CONV_2D(T#187, T#95, T#50) -> [T#188]\n",
      "  Op#38 ADD(T#188, T#184) -> [T#189]\n",
      "  Op#39 CONV_2D(T#189, T#96, T#51) -> [T#190]\n",
      "  Op#40 SHAPE(T#189) -> [T#191]\n",
      "GPU COMPATIBILITY WARNING: Not supported op SHAPE\n",
      "  Op#41 STRIDED_SLICE(T#191, T#143[1], T#144[2], T#143[1]) -> [T#192]\n",
      "GPU COMPATIBILITY WARNING: Slice does not support shrink_axis_mask parameter. \n",
      "  Op#42 STRIDED_SLICE(T#191, T#144[2], T#145[3], T#143[1]) -> [T#193]\n",
      "GPU COMPATIBILITY WARNING: Slice does not support shrink_axis_mask parameter. \n",
      "  Op#43 PACK(T#192, T#193) -> [T#194]\n",
      "  Op#44 CONV_2D(T#189, T#97, T#52) -> [T#195]\n",
      "  Op#45 CONV_2D(T#189, T#98, T#16) -> [T#196]\n",
      "  Op#46 PAD(T#196, T#149[0, 0, 1, 1, 1, ...]) -> [T#197]\n",
      "  Op#47 CONV_2D(T#197, T#99, T#17) -> [T#198]\n",
      "  Op#48 CONV_2D(T#198, T#100, T#53) -> [T#199]\n",
      "  Op#49 ADD(T#199, T#195) -> [T#200]\n",
      "  Op#50 CONV_2D(T#200, T#101, T#18) -> [T#201]\n",
      "  Op#51 PAD(T#201, T#149[0, 0, 1, 1, 1, ...]) -> [T#202]\n",
      "  Op#52 CONV_2D(T#202, T#102, T#19) -> [T#203]\n",
      "  Op#53 CONV_2D(T#203, T#103, T#54) -> [T#204]\n",
      "  Op#54 ADD(T#204, T#200) -> [T#205]\n",
      "  Op#55 CONV_2D(T#205, T#104, T#20) -> [T#206]\n",
      "  Op#56 PAD(T#206, T#149[0, 0, 1, 1, 1, ...]) -> [T#207]\n",
      "  Op#57 CONV_2D(T#207, T#105, T#21) -> [T#208]\n",
      "  Op#58 CONV_2D(T#208, T#106, T#55) -> [T#209]\n",
      "  Op#59 ADD(T#209, T#205) -> [T#210]\n",
      "  Op#60 CONV_2D(T#210, T#107, T#22) -> [T#211]\n",
      "  Op#61 PAD(T#211, T#149[0, 0, 1, 1, 1, ...]) -> [T#212]\n",
      "  Op#62 CONV_2D(T#212, T#108, T#23) -> [T#213]\n",
      "  Op#63 CONV_2D(T#213, T#109, T#56) -> [T#214]\n",
      "  Op#64 ADD(T#214, T#210) -> [T#215]\n",
      "  Op#65 CONV_2D(T#215, T#110, T#24) -> [T#216]\n",
      "  Op#66 PAD(T#216, T#149[0, 0, 1, 1, 1, ...]) -> [T#217]\n",
      "  Op#67 CONV_2D(T#217, T#111, T#25) -> [T#218]\n",
      "  Op#68 CONV_2D(T#218, T#112, T#57) -> [T#219]\n",
      "  Op#69 ADD(T#219, T#215) -> [T#220]\n",
      "  Op#70 CONV_2D(T#220, T#113, T#26) -> [T#221]\n",
      "  Op#71 PAD(T#221, T#149[0, 0, 1, 1, 1, ...]) -> [T#222]\n",
      "  Op#72 CONV_2D(T#222, T#114, T#27) -> [T#223]\n",
      "  Op#73 CONV_2D(T#223, T#115, T#58) -> [T#224]\n",
      "  Op#74 ADD(T#224, T#220) -> [T#225]\n",
      "  Op#75 CONV_2D(T#225, T#116, T#59) -> [T#226]\n",
      "  Op#76 SHAPE(T#225) -> [T#227]\n",
      "GPU COMPATIBILITY WARNING: Not supported op SHAPE\n",
      "  Op#77 STRIDED_SLICE(T#227, T#143[1], T#144[2], T#143[1]) -> [T#228]\n",
      "GPU COMPATIBILITY WARNING: Slice does not support shrink_axis_mask parameter. \n",
      "  Op#78 STRIDED_SLICE(T#227, T#144[2], T#145[3], T#143[1]) -> [T#229]\n",
      "GPU COMPATIBILITY WARNING: Slice does not support shrink_axis_mask parameter. \n",
      "  Op#79 PACK(T#228, T#229) -> [T#230]\n",
      "  Op#80 CONV_2D(T#225, T#117, T#60) -> [T#231]\n",
      "  Op#81 CONV_2D(T#225, T#118, T#28) -> [T#232]\n",
      "  Op#82 PAD(T#232, T#149[0, 0, 1, 1, 1, ...]) -> [T#233]\n",
      "  Op#83 CONV_2D(T#233, T#119, T#29) -> [T#234]\n",
      "  Op#84 CONV_2D(T#234, T#120, T#61) -> [T#235]\n",
      "  Op#85 ADD(T#235, T#231) -> [T#236]\n",
      "  Op#86 CONV_2D(T#236, T#121, T#30) -> [T#237]\n",
      "  Op#87 PAD(T#237, T#149[0, 0, 1, 1, 1, ...]) -> [T#238]\n",
      "  Op#88 CONV_2D(T#238, T#122, T#31) -> [T#239]\n",
      "  Op#89 CONV_2D(T#239, T#123, T#62) -> [T#240]\n",
      "  Op#90 ADD(T#240, T#236) -> [T#241]\n",
      "  Op#91 CONV_2D(T#241, T#124, T#32) -> [T#242]\n",
      "  Op#92 PAD(T#242, T#149[0, 0, 1, 1, 1, ...]) -> [T#243]\n",
      "  Op#93 CONV_2D(T#243, T#125, T#33) -> [T#244]\n",
      "  Op#94 CONV_2D(T#244, T#126, T#63) -> [T#245]\n",
      "  Op#95 ADD(T#245, T#241) -> [T#246]\n",
      "  Op#96 CONV_2D(T#246, T#127, T#64) -> [T#247]\n",
      "  Op#97 CONV_2D(T#247, T#128, T#65) -> [T#248]\n",
      "  Op#98 CONV_2D(T#248, T#129, T#34) -> [T#249]\n",
      "  Op#99 CONV_2D(T#249, T#130, T#35) -> [T#250]\n",
      "  Op#100 CONV_2D(T#250, T#131, T#36) -> [T#251]\n",
      "  Op#101 CONV_2D(T#251, T#132, T#37) -> [T#252]\n",
      "  Op#102 CONV_2D(T#252, T#133, T#66) -> [T#253]\n",
      "  Op#103 SHAPE(T#253) -> [T#254]\n",
      "GPU COMPATIBILITY WARNING: Not supported op SHAPE\n",
      "  Op#104 STRIDED_SLICE(T#254, T#148[0], T#143[1], T#143[1]) -> [T#255]\n",
      "GPU COMPATIBILITY WARNING: Slice does not support shrink_axis_mask parameter. \n",
      "  Op#105 PACK(T#255, T#146[-1], T#147[2]) -> [T#256]\n",
      "  Op#106 LOGISTIC(T#253) -> [T#257]\n",
      "  Op#107 RESHAPE(T#257, T#256) -> [T#258]\n",
      "GPU COMPATIBILITY WARNING: Expected 1 runtime input tensor(s), but node has 2 runtime input(s).\n",
      "  Op#108 CONV_2D(T#248, T#134, T#38) -> [T#259]\n",
      "  Op#109 CONV_2D(T#259, T#135, T#39) -> [T#260]\n",
      "  Op#110 CONV_2D(T#260, T#136, T#40) -> [T#261]\n",
      "  Op#111 CONV_2D(T#261, T#137, T#41) -> [T#262]\n",
      "  Op#112 CONV_2D(T#262, T#138, T#67) -> [T#263]\n",
      "  Op#113 SHAPE(T#263) -> [T#264]\n",
      "GPU COMPATIBILITY WARNING: Not supported op SHAPE\n",
      "  Op#114 STRIDED_SLICE(T#264, T#148[0], T#143[1], T#143[1]) -> [T#265]\n",
      "GPU COMPATIBILITY WARNING: Slice does not support shrink_axis_mask parameter. \n",
      "  Op#115 PACK(T#265, T#146[-1], T#150[4]) -> [T#266]\n",
      "  Op#116 RESHAPE(T#263, T#266) -> [T#267]\n",
      "GPU COMPATIBILITY WARNING: Expected 1 runtime input tensor(s), but node has 2 runtime input(s).\n",
      "  Op#117 RESIZE_NEAREST_NEIGHBOR(T#247, T#230) -> [T#268]\n",
      "GPU COMPATIBILITY WARNING: Expected 1 runtime input tensor(s), but node has 2 runtime input(s).\n",
      "  Op#118 ADD(T#268, T#226) -> [T#269]\n",
      "  Op#119 CONV_2D(T#269, T#139, T#68) -> [T#270]\n",
      "  Op#120 CONV_2D(T#270, T#129, T#34) -> [T#271]\n",
      "  Op#121 CONV_2D(T#271, T#130, T#35) -> [T#272]\n",
      "  Op#122 CONV_2D(T#272, T#131, T#36) -> [T#273]\n",
      "  Op#123 CONV_2D(T#273, T#132, T#37) -> [T#274]\n",
      "  Op#124 CONV_2D(T#274, T#133, T#66) -> [T#275]\n",
      "  Op#125 SHAPE(T#275) -> [T#276]\n",
      "GPU COMPATIBILITY WARNING: Not supported op SHAPE\n",
      "  Op#126 STRIDED_SLICE(T#276, T#148[0], T#143[1], T#143[1]) -> [T#277]\n",
      "GPU COMPATIBILITY WARNING: Slice does not support shrink_axis_mask parameter. \n",
      "  Op#127 PACK(T#277, T#146[-1], T#147[2]) -> [T#278]\n",
      "  Op#128 LOGISTIC(T#275) -> [T#279]\n",
      "  Op#129 RESHAPE(T#279, T#278) -> [T#280]\n",
      "GPU COMPATIBILITY WARNING: Expected 1 runtime input tensor(s), but node has 2 runtime input(s).\n",
      "  Op#130 CONV_2D(T#270, T#134, T#38) -> [T#281]\n",
      "  Op#131 CONV_2D(T#281, T#135, T#39) -> [T#282]\n",
      "  Op#132 CONV_2D(T#282, T#136, T#40) -> [T#283]\n",
      "  Op#133 CONV_2D(T#283, T#137, T#41) -> [T#284]\n",
      "  Op#134 CONV_2D(T#284, T#138, T#67) -> [T#285]\n",
      "  Op#135 SHAPE(T#285) -> [T#286]\n",
      "GPU COMPATIBILITY WARNING: Not supported op SHAPE\n",
      "  Op#136 STRIDED_SLICE(T#286, T#148[0], T#143[1], T#143[1]) -> [T#287]\n",
      "GPU COMPATIBILITY WARNING: Slice does not support shrink_axis_mask parameter. \n",
      "  Op#137 PACK(T#287, T#146[-1], T#150[4]) -> [T#288]\n",
      "  Op#138 RESHAPE(T#285, T#288) -> [T#289]\n",
      "GPU COMPATIBILITY WARNING: Expected 1 runtime input tensor(s), but node has 2 runtime input(s).\n",
      "  Op#139 RESIZE_NEAREST_NEIGHBOR(T#269, T#194) -> [T#290]\n",
      "GPU COMPATIBILITY WARNING: Expected 1 runtime input tensor(s), but node has 2 runtime input(s).\n",
      "  Op#140 ADD(T#290, T#190) -> [T#291]\n",
      "  Op#141 CONV_2D(T#291, T#140, T#69) -> [T#292]\n",
      "  Op#142 CONV_2D(T#292, T#129, T#34) -> [T#293]\n",
      "  Op#143 CONV_2D(T#293, T#130, T#35) -> [T#294]\n",
      "  Op#144 CONV_2D(T#294, T#131, T#36) -> [T#295]\n",
      "  Op#145 CONV_2D(T#295, T#132, T#37) -> [T#296]\n",
      "  Op#146 CONV_2D(T#296, T#133, T#66) -> [T#297]\n",
      "  Op#147 SHAPE(T#297) -> [T#298]\n",
      "GPU COMPATIBILITY WARNING: Not supported op SHAPE\n",
      "  Op#148 STRIDED_SLICE(T#298, T#148[0], T#143[1], T#143[1]) -> [T#299]\n",
      "GPU COMPATIBILITY WARNING: Slice does not support shrink_axis_mask parameter. \n",
      "  Op#149 PACK(T#299, T#146[-1], T#147[2]) -> [T#300]\n",
      "  Op#150 LOGISTIC(T#297) -> [T#301]\n",
      "  Op#151 RESHAPE(T#301, T#300) -> [T#302]\n",
      "GPU COMPATIBILITY WARNING: Expected 1 runtime input tensor(s), but node has 2 runtime input(s).\n",
      "  Op#152 CONV_2D(T#292, T#134, T#38) -> [T#303]\n",
      "  Op#153 CONV_2D(T#303, T#135, T#39) -> [T#304]\n",
      "  Op#154 CONV_2D(T#304, T#136, T#40) -> [T#305]\n",
      "  Op#155 CONV_2D(T#305, T#137, T#41) -> [T#306]\n",
      "  Op#156 CONV_2D(T#306, T#138, T#67) -> [T#307]\n",
      "  Op#157 SHAPE(T#307) -> [T#308]\n",
      "GPU COMPATIBILITY WARNING: Not supported op SHAPE\n",
      "  Op#158 STRIDED_SLICE(T#308, T#148[0], T#143[1], T#143[1]) -> [T#309]\n",
      "GPU COMPATIBILITY WARNING: Slice does not support shrink_axis_mask parameter. \n",
      "  Op#159 PACK(T#309, T#146[-1], T#150[4]) -> [T#310]\n",
      "  Op#160 RESHAPE(T#307, T#310) -> [T#311]\n",
      "GPU COMPATIBILITY WARNING: Expected 1 runtime input tensor(s), but node has 2 runtime input(s).\n",
      "  Op#161 CONV_2D(T#246, T#141, T#70) -> [T#312]\n",
      "  Op#162 RELU(T#312) -> [T#313]\n",
      "  Op#163 CONV_2D(T#313, T#142, T#71) -> [T#314]\n",
      "  Op#164 CONV_2D(T#314, T#129, T#34) -> [T#315]\n",
      "  Op#165 CONV_2D(T#315, T#130, T#35) -> [T#316]\n",
      "  Op#166 CONV_2D(T#316, T#131, T#36) -> [T#317]\n",
      "  Op#167 CONV_2D(T#317, T#132, T#37) -> [T#318]\n",
      "  Op#168 CONV_2D(T#318, T#133, T#66) -> [T#319]\n",
      "  Op#169 SHAPE(T#319) -> [T#320]\n",
      "GPU COMPATIBILITY WARNING: Not supported op SHAPE\n",
      "  Op#170 STRIDED_SLICE(T#320, T#148[0], T#143[1], T#143[1]) -> [T#321]\n",
      "GPU COMPATIBILITY WARNING: Slice does not support shrink_axis_mask parameter. \n",
      "  Op#171 PACK(T#321, T#146[-1], T#147[2]) -> [T#322]\n",
      "  Op#172 LOGISTIC(T#319) -> [T#323]\n",
      "  Op#173 RESHAPE(T#323, T#322) -> [T#324]\n",
      "GPU COMPATIBILITY WARNING: Expected 1 runtime input tensor(s), but node has 2 runtime input(s).\n",
      "  Op#174 CONV_2D(T#314, T#134, T#38) -> [T#325]\n",
      "  Op#175 CONV_2D(T#325, T#135, T#39) -> [T#326]\n",
      "  Op#176 CONV_2D(T#326, T#136, T#40) -> [T#327]\n",
      "  Op#177 CONV_2D(T#327, T#137, T#41) -> [T#328]\n",
      "  Op#178 CONV_2D(T#328, T#138, T#67) -> [T#329]\n",
      "  Op#179 SHAPE(T#329) -> [T#330]\n",
      "GPU COMPATIBILITY WARNING: Not supported op SHAPE\n",
      "  Op#180 STRIDED_SLICE(T#330, T#148[0], T#143[1], T#143[1]) -> [T#331]\n",
      "GPU COMPATIBILITY WARNING: Slice does not support shrink_axis_mask parameter. \n",
      "  Op#181 PACK(T#331, T#146[-1], T#150[4]) -> [T#332]\n",
      "  Op#182 RESHAPE(T#329, T#332) -> [T#333]\n",
      "GPU COMPATIBILITY WARNING: Expected 1 runtime input tensor(s), but node has 2 runtime input(s).\n",
      "  Op#183 CONV_2D(T#312, T#129, T#34) -> [T#334]\n",
      "  Op#184 CONV_2D(T#334, T#130, T#35) -> [T#335]\n",
      "  Op#185 CONV_2D(T#335, T#131, T#36) -> [T#336]\n",
      "  Op#186 CONV_2D(T#336, T#132, T#37) -> [T#337]\n",
      "  Op#187 CONV_2D(T#337, T#133, T#66) -> [T#338]\n",
      "  Op#188 SHAPE(T#338) -> [T#339]\n",
      "GPU COMPATIBILITY WARNING: Not supported op SHAPE\n",
      "  Op#189 STRIDED_SLICE(T#339, T#148[0], T#143[1], T#143[1]) -> [T#340]\n",
      "GPU COMPATIBILITY WARNING: Slice does not support shrink_axis_mask parameter. \n",
      "  Op#190 PACK(T#340, T#146[-1], T#147[2]) -> [T#341]\n",
      "  Op#191 LOGISTIC(T#338) -> [T#342]\n",
      "  Op#192 RESHAPE(T#342, T#341) -> [T#343]\n",
      "GPU COMPATIBILITY WARNING: Expected 1 runtime input tensor(s), but node has 2 runtime input(s).\n",
      "  Op#193 CONCATENATION(T#302, T#280, T#258, T#343, T#324) -> [T#344]\n",
      "  Op#194 CONV_2D(T#312, T#134, T#38) -> [T#345]\n",
      "  Op#195 CONV_2D(T#345, T#135, T#39) -> [T#346]\n",
      "  Op#196 CONV_2D(T#346, T#136, T#40) -> [T#347]\n",
      "  Op#197 CONV_2D(T#347, T#137, T#41) -> [T#348]\n",
      "  Op#198 CONV_2D(T#348, T#138, T#67) -> [T#349]\n",
      "  Op#199 SHAPE(T#349) -> [T#350]\n",
      "GPU COMPATIBILITY WARNING: Not supported op SHAPE\n",
      "  Op#200 STRIDED_SLICE(T#350, T#148[0], T#143[1], T#143[1]) -> [T#351]\n",
      "GPU COMPATIBILITY WARNING: Slice does not support shrink_axis_mask parameter. \n",
      "  Op#201 PACK(T#351, T#146[-1], T#150[4]) -> [T#352]\n",
      "  Op#202 RESHAPE(T#349, T#352) -> [T#353]\n",
      "GPU COMPATIBILITY WARNING: Expected 1 runtime input tensor(s), but node has 2 runtime input(s).\n",
      "  Op#203 CONCATENATION(T#311, T#289, T#267, T#353, T#333) -> [T#354]\n",
      "\n",
      "GPU COMPATIBILITY WARNING: Subgraph#0 has GPU delegate compatibility issues at nodes 40, 41, 42, 76, 77, 78, 103, 104, 107, 113, 114, 116, 117, 125, 126, 129, 135, 136, 138, 139, 147, 148, 151, 157, 158, 160, 169, 170, 173, 179, 180, 182, 188, 189, 192, 199, 200, 202 on TFLite runtime version 2.15.0\n",
      "\n",
      "Tensors of Subgraph#0\n",
      "  T#0(serving_default_input_1:0) shape_signature:[-1, -1, -1, 3], type:FLOAT32\n",
      "  T#1(retinanet/bn_conv1/FusedBatchNormV3) shape:[64], type:FLOAT32 RO 256 bytes, buffer: 2, data:[1.48173, -1.10685, -1.05162, -1.90365, 1.33259, ...]\n",
      "  T#2(retinanet/bn2a_branch2a/FusedBatchNormV3) shape:[64], type:FLOAT32 RO 256 bytes, buffer: 3, data:[3.55286, 3.54701, 2.03126, -4.40384, 5.70318, ...]\n",
      "  T#3(retinanet/bn2a_branch2b/FusedBatchNormV3) shape:[64], type:FLOAT32 RO 256 bytes, buffer: 4, data:[-2.03306, 0.507871, 0.245214, 4.26877, -2.25404, ...]\n",
      "  T#4(retinanet/bn2b_branch2a/FusedBatchNormV3) shape:[64], type:FLOAT32 RO 256 bytes, buffer: 5, data:[0.40411, 0.529509, 0.659124, 0.359537, 1.07271, ...]\n",
      "  T#5(retinanet/bn2b_branch2b/FusedBatchNormV3) shape:[64], type:FLOAT32 RO 256 bytes, buffer: 6, data:[-0.566339, 2.19773, 0.469075, 0.354645, 0.671897, ...]\n",
      "  T#6(retinanet/bn2c_branch2a/FusedBatchNormV3) shape:[64], type:FLOAT32 RO 256 bytes, buffer: 7, data:[1.54701, 0.445305, -0.207709, 0.0857184, -1.74178, ...]\n",
      "  T#7(retinanet/bn2c_branch2b/FusedBatchNormV3) shape:[64], type:FLOAT32 RO 256 bytes, buffer: 8, data:[-0.410776, 1.8384, 2.24939, 0.168918, -0.136441, ...]\n",
      "  T#8(retinanet/bn3a_branch2a/FusedBatchNormV3) shape:[128], type:FLOAT32 RO 512 bytes, buffer: 9, data:[1.88629, 0.129865, -0.533765, -0.326179, 0.851805, ...]\n",
      "  T#9(retinanet/bn3a_branch2b/FusedBatchNormV3) shape:[128], type:FLOAT32 RO 512 bytes, buffer: 10, data:[0.355914, 0.364394, -0.721266, -1.23132, 4.36128, ...]\n",
      "  T#10(retinanet/bn3b_branch2a/FusedBatchNormV3) shape:[128], type:FLOAT32 RO 512 bytes, buffer: 11, data:[-0.35221, -1.23721, 0.466849, -2.37233, -1.61141, ...]\n",
      "  T#11(retinanet/bn3b_branch2b/FusedBatchNormV3) shape:[128], type:FLOAT32 RO 512 bytes, buffer: 12, data:[-0.259494, -0.239017, 0.470877, 1.12552, -0.431811, ...]\n",
      "  T#12(retinanet/bn3c_branch2a/FusedBatchNormV3) shape:[128], type:FLOAT32 RO 512 bytes, buffer: 13, data:[-1.11817, -0.409156, -0.590121, -0.642856, -0.23992, ...]\n",
      "  T#13(retinanet/bn3c_branch2b/FusedBatchNormV3) shape:[128], type:FLOAT32 RO 512 bytes, buffer: 14, data:[-1.1158, -0.26023, 0.796593, -0.88581, -2.46931, ...]\n",
      "  T#14(retinanet/bn3d_branch2a/FusedBatchNormV3) shape:[128], type:FLOAT32 RO 512 bytes, buffer: 15, data:[-0.0166816, -0.433818, 0.708005, 0.532921, 0.178296, ...]\n",
      "  T#15(retinanet/bn3d_branch2b/FusedBatchNormV3) shape:[128], type:FLOAT32 RO 512 bytes, buffer: 16, data:[0.879653, -0.484479, 0.458192, 0.419282, 0.481374, ...]\n",
      "  T#16(retinanet/bn4a_branch2a/FusedBatchNormV3) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 17, data:[-0.120672, 0.912179, -1.80784, -0.433978, -0.874602, ...]\n",
      "  T#17(retinanet/bn4a_branch2b/FusedBatchNormV3) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 18, data:[0.220543, -0.347756, 0.301897, 0.715676, -0.449679, ...]\n",
      "  T#18(retinanet/bn4b_branch2a/FusedBatchNormV3) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 19, data:[1.0946, 0.924734, -0.573882, 0.905708, 0.166484, ...]\n",
      "  T#19(retinanet/bn4b_branch2b/FusedBatchNormV3) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 20, data:[-0.208689, -0.723039, 2.90526, 1.14683, 0.543231, ...]\n",
      "  T#20(retinanet/bn4c_branch2a/FusedBatchNormV3) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 21, data:[0.303153, 0.325072, -0.187505, -0.792597, -0.780077, ...]\n",
      "  T#21(retinanet/bn4c_branch2b/FusedBatchNormV3) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 22, data:[0.41055, -1.21301, 0.787572, 0.713107, -0.230492, ...]\n",
      "  T#22(retinanet/bn4d_branch2a/FusedBatchNormV3) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 23, data:[-0.390297, 0.360375, 0.301622, -0.670661, -0.567542, ...]\n",
      "  T#23(retinanet/bn4d_branch2b/FusedBatchNormV3) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 24, data:[-0.611168, 0.208689, -0.249035, 0.235631, 0.0360592, ...]\n",
      "  T#24(retinanet/bn4e_branch2a/FusedBatchNormV3) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 25, data:[-0.282347, 0.203594, 0.0258506, -0.47026, -0.645411, ...]\n",
      "  T#25(retinanet/bn4e_branch2b/FusedBatchNormV3) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 26, data:[0.451313, 0.671004, -1.92686, 0.602529, 0.494453, ...]\n",
      "  T#26(retinanet/bn4f_branch2a/FusedBatchNormV3) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 27, data:[-1.15927, -1.21406, -0.7812, -0.721321, -0.461613, ...]\n",
      "  T#27(retinanet/bn4f_branch2b/FusedBatchNormV3) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 28, data:[0.55179, 0.492208, 0.34386, 0.373528, -0.165827, ...]\n",
      "  T#28(retinanet/bn5a_branch2a/FusedBatchNormV3) shape:[512], type:FLOAT32 RO 2048 bytes, buffer: 29, data:[-0.299787, -0.234434, -0.896497, -0.0010764, 0.676991, ...]\n",
      "  T#29(retinanet/bn5a_branch2b/FusedBatchNormV3) shape:[512], type:FLOAT32 RO 2048 bytes, buffer: 30, data:[0.356321, 0.180877, 0.263071, 0.200317, 0.701227, ...]\n",
      "  T#30(retinanet/bn5b_branch2a/FusedBatchNormV3) shape:[512], type:FLOAT32 RO 2048 bytes, buffer: 31, data:[0.321595, 0.0321113, -0.605777, 0.289451, 0.153169, ...]\n",
      "  T#31(retinanet/bn5b_branch2b/FusedBatchNormV3) shape:[512], type:FLOAT32 RO 2048 bytes, buffer: 32, data:[0.169187, 0.110812, 0.334952, -0.182769, 0.430846, ...]\n",
      "  T#32(retinanet/bn5c_branch2a/FusedBatchNormV3) shape:[512], type:FLOAT32 RO 2048 bytes, buffer: 33, data:[0.0104173, -0.419171, -0.143356, -0.187384, -0.282172, ...]\n",
      "  T#33(retinanet/bn5c_branch2b/FusedBatchNormV3) shape:[512], type:FLOAT32 RO 2048 bytes, buffer: 34, data:[-0.0217479, 0.296553, 0.115487, -0.0475037, 0.406063, ...]\n",
      "  T#34(retinanet/classification_submodel/pyramid_classification_0/BiasAdd/ReadVariableOp) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 35, data:[-0.00738146, 0.00324688, -0.0141069, 0.000888787, -0.00744733, ...]\n",
      "  T#35(retinanet/classification_submodel/pyramid_classification_1/BiasAdd/ReadVariableOp) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 36, data:[-0.00409133, -0.000629348, -0.00787874, -0.0103418, -0.0126532, ...]\n",
      "  T#36(retinanet/classification_submodel/pyramid_classification_2/BiasAdd/ReadVariableOp) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 37, data:[-0.00602476, -0.00480516, -0.0107582, -0.00691397, -0.0115202, ...]\n",
      "  T#37(retinanet/classification_submodel/pyramid_classification_3/BiasAdd/ReadVariableOp) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 38, data:[-0.00725418, 0.0019315, -0.0102986, -0.00916284, -0.00576712, ...]\n",
      "  T#38(retinanet/regression_submodel/pyramid_regression_0/BiasAdd/ReadVariableOp) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 39, data:[-0.0120305, -0.0150981, -0.0126247, -0.00734283, -0.0140576, ...]\n",
      "  T#39(retinanet/regression_submodel/pyramid_regression_1/BiasAdd/ReadVariableOp) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 40, data:[-0.00408741, -0.00837329, -0.0019846, -0.00313111, -0.0078649, ...]\n",
      "  T#40(retinanet/regression_submodel/pyramid_regression_2/BiasAdd/ReadVariableOp) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 41, data:[0.00325775, 0.00131129, -0.010097, -0.00818296, -0.00427845, ...]\n",
      "  T#41(retinanet/regression_submodel/pyramid_regression_3/BiasAdd/ReadVariableOp) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 42, data:[-0.00937556, -0.0117189, -0.00557553, 0.00777173, -0.0021628, ...]\n",
      "  T#42(retinanet/bn2a_branch1/FusedBatchNormV3) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 43, data:[-0.098058, -1.78322, 2.74794, 0.948467, 2.5567, ...]\n",
      "  T#43(retinanet/bn2a_branch2c/FusedBatchNormV3) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 44, data:[-1.36381, -0.124947, 3.23563, -2.8662, 2.62692, ...]\n",
      "  T#44(retinanet/bn2b_branch2c/FusedBatchNormV3) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 45, data:[-3.55673, -0.626014, 0.103755, -0.745718, -2.00939, ...]\n",
      "  T#45(retinanet/bn2c_branch2c/FusedBatchNormV3) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 46, data:[0.0837505, 1.56937, 0.0281556, -0.248643, -1.25644, ...]\n",
      "  T#46(retinanet/bn3a_branch1/FusedBatchNormV3) shape:[512], type:FLOAT32 RO 2048 bytes, buffer: 47, data:[1.00127, -0.569048, 0.58966, 1.85851, 0.299977, ...]\n",
      "  T#47(retinanet/bn3a_branch2c/FusedBatchNormV3) shape:[512], type:FLOAT32 RO 2048 bytes, buffer: 48, data:[-0.0642706, 0.71305, -0.132087, 0.886226, 0.858062, ...]\n",
      "  T#48(retinanet/bn3b_branch2c/FusedBatchNormV3) shape:[512], type:FLOAT32 RO 2048 bytes, buffer: 49, data:[-0.106625, -0.259431, -0.782798, -0.275508, -0.114607, ...]\n",
      "  T#49(retinanet/bn3c_branch2c/FusedBatchNormV3) shape:[512], type:FLOAT32 RO 2048 bytes, buffer: 50, data:[-0.450397, -0.729809, -0.335053, 0.122768, 0.0110706, ...]\n",
      "  T#50(retinanet/bn3d_branch2c/FusedBatchNormV3) shape:[512], type:FLOAT32 RO 2048 bytes, buffer: 51, data:[0.818328, 1.63834, -0.551447, 0.14907, 0.0347946, ...]\n",
      "  T#51(retinanet/C3_reduced/BiasAdd/ReadVariableOp) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 52, data:[-0.00474641, 0.00379222, 0.0139902, 0.00260797, -0.00774952, ...]\n",
      "  T#52(retinanet/bn4a_branch1/FusedBatchNormV3) shape:[1024], type:FLOAT32 RO 4096 bytes, buffer: 53, data:[0.777251, -0.605599, -2.20143, -1.13066, -0.143847, ...]\n",
      "  T#53(retinanet/bn4a_branch2c/FusedBatchNormV3) shape:[1024], type:FLOAT32 RO 4096 bytes, buffer: 54, data:[-0.691701, -0.871359, 0.228672, 0.871771, 1.49988, ...]\n",
      "  T#54(retinanet/bn4b_branch2c/FusedBatchNormV3) shape:[1024], type:FLOAT32 RO 4096 bytes, buffer: 55, data:[-0.419811, 0.235025, -0.585914, -0.592246, -0.0736609, ...]\n",
      "  T#55(retinanet/bn4c_branch2c/FusedBatchNormV3) shape:[1024], type:FLOAT32 RO 4096 bytes, buffer: 56, data:[-0.145606, 0.2614, -0.638123, -0.102457, 0.107266, ...]\n",
      "  T#56(retinanet/bn4d_branch2c/FusedBatchNormV3) shape:[1024], type:FLOAT32 RO 4096 bytes, buffer: 57, data:[0.113025, -0.294592, -0.233621, 0.220572, 0.280968, ...]\n",
      "  T#57(retinanet/bn4e_branch2c/FusedBatchNormV3) shape:[1024], type:FLOAT32 RO 4096 bytes, buffer: 58, data:[-0.318242, -0.564207, 0.0739433, -0.133637, 0.410457, ...]\n",
      "  T#58(retinanet/bn4f_branch2c/FusedBatchNormV3) shape:[1024], type:FLOAT32 RO 4096 bytes, buffer: 59, data:[0.565651, 0.498879, -0.0124655, -0.2322, -0.0246157, ...]\n",
      "  T#59(retinanet/C4_reduced/BiasAdd/ReadVariableOp) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 60, data:[0.00405675, 0.00216405, 0.0109937, 0.00343797, -0.00351217, ...]\n",
      "  T#60(retinanet/bn5a_branch1/FusedBatchNormV3) shape:[2048], type:FLOAT32 RO 8192 bytes, buffer: 61, data:[0.229141, -1.43986, 0.559014, -1.2837, -1.20576, ...]\n",
      "  T#61(retinanet/bn5a_branch2c/FusedBatchNormV3) shape:[2048], type:FLOAT32 RO 8192 bytes, buffer: 62, data:[-0.611617, 1.93481, -1.27421, -0.375263, -0.0708447, ...]\n",
      "  T#62(retinanet/bn5b_branch2c/FusedBatchNormV3) shape:[2048], type:FLOAT32 RO 8192 bytes, buffer: 63, data:[-0.0886213, -1.69599, -1.15829, -0.368562, -1.03176, ...]\n",
      "  T#63(retinanet/bn5c_branch2c/FusedBatchNormV3) shape:[2048], type:FLOAT32 RO 8192 bytes, buffer: 64, data:[0.27843, -0.410798, -0.615281, -0.617931, -0.574998, ...]\n",
      "  T#64(retinanet/C5_reduced/BiasAdd/ReadVariableOp) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 65, data:[-0.00441945, -0.00860969, 0.0120631, -0.006782, 0.000705592, ...]\n",
      "  T#65(retinanet/P5/BiasAdd/ReadVariableOp) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 66, data:[-0.0199726, -0.0164499, 0.00879194, -0.0119754, 0.0126696, ...]\n",
      "  T#66(retinanet/classification_submodel/pyramid_classification/BiasAdd/ReadVariableOp) shape:[18], type:FLOAT32 RO 72 bytes, buffer: 67, data:[-4.60289, -4.59737, -4.59481, -4.58567, -4.59413, ...]\n",
      "  T#67(retinanet/regression_submodel/pyramid_regression/BiasAdd/ReadVariableOp) shape:[36], type:FLOAT32 RO 144 bytes, buffer: 68, data:[0.0057127, -0.0121145, -0.00568769, 0.0111851, 0.00679796, ...]\n",
      "  T#68(retinanet/P4/BiasAdd/ReadVariableOp) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 69, data:[-0.000141367, -0.00332181, 0.00446471, -0.0168057, 0.00703298, ...]\n",
      "  T#69(retinanet/P3/BiasAdd/ReadVariableOp) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 70, data:[-0.00527707, -0.00501145, 0.0125564, -0.0119048, 0.0040348, ...]\n",
      "  T#70(retinanet/P6/BiasAdd/ReadVariableOp) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 71, data:[-0.0046093, -0.000796947, 0.00289003, -0.00654701, -0.00185894, ...]\n",
      "  T#71(retinanet/P7/BiasAdd/ReadVariableOp) shape:[256], type:FLOAT32 RO 1024 bytes, buffer: 72, data:[-0.00398935, -0.0103957, 1.27241e-05, 0.00174002, 0.000253822, ...]\n",
      "  T#72(retinanet/conv1/Conv2D) shape:[64, 7, 7, 3], type:FLOAT32 RO 37632 bytes, buffer: 73, data:[0.00045493, 0.000137318, -0.000119683, 0.000306095, 0.000328543, ...]\n",
      "  T#73(retinanet/res2a_branch1/Conv2D) shape:[256, 1, 1, 64], type:FLOAT32 RO 65536 bytes, buffer: 74, data:[0.00135273, -0.00329863, 0.00171423, 0.00524716, -0.0145643, ...]\n",
      "  T#74(retinanet/res2a_branch2a/Conv2D) shape:[64, 1, 1, 64], type:FLOAT32 RO 16384 bytes, buffer: 75, data:[0.0793734, 0.00519125, -0.0240403, 0.0462823, -0.0103659, ...]\n",
      "  T#75(retinanet/res2a_branch2b/Conv2D) shape:[64, 3, 3, 64], type:FLOAT32 RO 147456 bytes, buffer: 76, data:[-0.0367353, 0.0149539, -0.0299165, 0.000872652, 0.00861516, ...]\n",
      "  T#76(retinanet/res2a_branch2c/Conv2D) shape:[256, 1, 1, 64], type:FLOAT32 RO 65536 bytes, buffer: 77, data:[-0.0172369, 0.0280928, -0.0431204, -0.0217609, -0.00464619, ...]\n",
      "  T#77(retinanet/res2b_branch2a/Conv2D) shape:[64, 1, 1, 256], type:FLOAT32 RO 65536 bytes, buffer: 78, data:[0.00320104, -0.00697935, -0.0175255, -0.0306621, -0.0200386, ...]\n",
      "  T#78(retinanet/res2b_branch2b/Conv2D) shape:[64, 3, 3, 64], type:FLOAT32 RO 147456 bytes, buffer: 79, data:[0.0217085, -0.0623047, -0.00836861, -0.0187818, 0.027968, ...]\n",
      "  T#79(retinanet/res2b_branch2c/Conv2D) shape:[256, 1, 1, 64], type:FLOAT32 RO 65536 bytes, buffer: 80, data:[-0.250108, -0.170552, 0.0428015, -0.281224, -0.637852, ...]\n",
      "  T#80(retinanet/res2c_branch2a/Conv2D) shape:[64, 1, 1, 256], type:FLOAT32 RO 65536 bytes, buffer: 81, data:[0.0128401, -0.044852, 0.044101, 0.0523888, -0.0647556, ...]\n",
      "  T#81(retinanet/res2c_branch2b/Conv2D) shape:[64, 3, 3, 64], type:FLOAT32 RO 147456 bytes, buffer: 82, data:[-0.00793682, 0.00744565, -0.00021131, 0.0561858, -0.0166352, ...]\n",
      "  T#82(retinanet/res2c_branch2c/Conv2D) shape:[256, 1, 1, 64], type:FLOAT32 RO 65536 bytes, buffer: 83, data:[-0.0309668, 0.153912, -0.18779, -0.0310272, -0.176632, ...]\n",
      "  T#83(retinanet/res3a_branch1/Conv2D) shape:[512, 1, 1, 256], type:FLOAT32 RO 524288 bytes, buffer: 84, data:[-0.0323684, -0.0248513, 0.121784, 0.0385428, -0.029875, ...]\n",
      "  T#84(retinanet/res3a_branch2a/Conv2D) shape:[128, 1, 1, 256], type:FLOAT32 RO 131072 bytes, buffer: 85, data:[0.0278347, 0.00275702, -0.0317323, 0.0109387, 0.00584018, ...]\n",
      "  T#85(retinanet/res3a_branch2b/Conv2D) shape:[128, 3, 3, 128], type:FLOAT32 RO 589824 bytes, buffer: 86, data:[-0.019007, 0.00580083, 0.0089192, 0.000675564, 0.00314025, ...]\n",
      "  T#86(retinanet/res3a_branch2c/Conv2D) shape:[512, 1, 1, 128], type:FLOAT32 RO 262144 bytes, buffer: 87, data:[-0.025074, -0.0539793, -0.108376, -0.0295581, 0.0869451, ...]\n",
      "  T#87(retinanet/res3b_branch2a/Conv2D) shape:[128, 1, 1, 512], type:FLOAT32 RO 262144 bytes, buffer: 88, data:[-0.02526, 0.0203277, 0.0165082, -0.0199036, 0.00666217, ...]\n",
      "  T#88(retinanet/res3b_branch2b/Conv2D) shape:[128, 3, 3, 128], type:FLOAT32 RO 589824 bytes, buffer: 89, data:[0.00328598, -0.0158388, -0.0577831, 0.0177959, 0.00192491, ...]\n",
      "  T#89(retinanet/res3b_branch2c/Conv2D) shape:[512, 1, 1, 128], type:FLOAT32 RO 262144 bytes, buffer: 90, data:[0.0360426, -0.0905636, 0.169969, -0.0580197, -0.0477945, ...]\n",
      "  T#90(retinanet/res3c_branch2a/Conv2D) shape:[128, 1, 1, 512], type:FLOAT32 RO 262144 bytes, buffer: 91, data:[0.0283311, 0.00838454, 0.0290843, -0.000687238, -0.015527, ...]\n",
      "  T#91(retinanet/res3c_branch2b/Conv2D) shape:[128, 3, 3, 128], type:FLOAT32 RO 589824 bytes, buffer: 92, data:[-0.0249932, 0.0279295, -0.0129992, -0.00278338, 0.0308702, ...]\n",
      "  T#92(retinanet/res3c_branch2c/Conv2D) shape:[512, 1, 1, 128], type:FLOAT32 RO 262144 bytes, buffer: 93, data:[0.0426841, -0.0554912, -0.0127802, 0.120163, -0.0016202, ...]\n",
      "  T#93(retinanet/res3d_branch2a/Conv2D) shape:[128, 1, 1, 512], type:FLOAT32 RO 262144 bytes, buffer: 94, data:[0.00161232, -0.0142077, 0.000117656, 0.0134925, 0.047593, ...]\n",
      "  T#94(retinanet/res3d_branch2b/Conv2D) shape:[128, 3, 3, 128], type:FLOAT32 RO 589824 bytes, buffer: 95, data:[-0.0309147, -0.0245009, -0.00366078, -0.0402972, -0.0279738, ...]\n",
      "  T#95(retinanet/res3d_branch2c/Conv2D) shape:[512, 1, 1, 128], type:FLOAT32 RO 262144 bytes, buffer: 96, data:[0.0302632, -0.00682848, 0.00209307, 0.0371228, 0.00184718, ...]\n",
      "  T#96(retinanet/C3_reduced/Conv2D) shape:[256, 1, 1, 512], type:FLOAT32 RO 524288 bytes, buffer: 97, data:[-0.0159414, 0.0673399, 0.0155901, 0.00362232, 0.0079056, ...]\n",
      "  T#97(retinanet/res4a_branch1/Conv2D) shape:[1024, 1, 1, 512], type:FLOAT32 RO 2097152 bytes, buffer: 98, data:[0.0390634, 0.00724973, 0.0127964, 0.110053, -0.00952661, ...]\n",
      "  T#98(retinanet/res4a_branch2a/Conv2D) shape:[256, 1, 1, 512], type:FLOAT32 RO 524288 bytes, buffer: 99, data:[-0.000870325, 0.0194413, -0.011824, 0.000387685, 0.00756236, ...]\n",
      "  T#99(retinanet/res4a_branch2b/Conv2D) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 100, data:[-0.0141506, -0.00200695, 0.00978525, 0.0103679, -0.014279, ...]\n",
      "  T#100(retinanet/res4a_branch2c/Conv2D) shape:[1024, 1, 1, 256], type:FLOAT32 RO 1048576 bytes, buffer: 101, data:[0.0677242, -0.0794119, 0.0442642, -0.0691226, -0.1168, ...]\n",
      "  T#101(retinanet/res4b_branch2a/Conv2D) shape:[256, 1, 1, 1024], type:FLOAT32 RO 1048576 bytes, buffer: 102, data:[-0.00474012, 0.00701958, -0.00747347, 0.00862804, -0.00845161, ...]\n",
      "  T#102(retinanet/res4b_branch2b/Conv2D) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 103, data:[-0.0411881, 0.00753136, 0.00958907, 0.0142146, 0.0575636, ...]\n",
      "  T#103(retinanet/res4b_branch2c/Conv2D) shape:[1024, 1, 1, 256], type:FLOAT32 RO 1048576 bytes, buffer: 104, data:[0.042628, 0.0930504, 0.00547063, -0.013982, -0.110472, ...]\n",
      "  T#104(retinanet/res4c_branch2a/Conv2D) shape:[256, 1, 1, 1024], type:FLOAT32 RO 1048576 bytes, buffer: 105, data:[-0.000903498, -0.0157438, 0.0037016, -0.0104776, 0.0126066, ...]\n",
      "  T#105(retinanet/res4c_branch2b/Conv2D) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 106, data:[0.0431902, 0.0146449, -0.00142783, -0.0140727, -0.0870318, ...]\n",
      "  T#106(retinanet/res4c_branch2c/Conv2D) shape:[1024, 1, 1, 256], type:FLOAT32 RO 1048576 bytes, buffer: 107, data:[-0.0781623, -0.0249666, 0.0417412, 0.0409146, 0.228936, ...]\n",
      "  T#107(retinanet/res4d_branch2a/Conv2D) shape:[256, 1, 1, 1024], type:FLOAT32 RO 1048576 bytes, buffer: 108, data:[-0.0115574, -0.00697571, -0.00765982, 0.00890531, 0.0848292, ...]\n",
      "  T#108(retinanet/res4d_branch2b/Conv2D) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 109, data:[0.0200792, -0.0014109, -0.0109304, -0.0181397, 0.0294776, ...]\n",
      "  T#109(retinanet/res4d_branch2c/Conv2D) shape:[1024, 1, 1, 256], type:FLOAT32 RO 1048576 bytes, buffer: 110, data:[0.139977, -0.0473388, -0.128771, -0.0600073, 0.0324212, ...]\n",
      "  T#110(retinanet/res4e_branch2a/Conv2D) shape:[256, 1, 1, 1024], type:FLOAT32 RO 1048576 bytes, buffer: 111, data:[-0.0386369, -0.0078681, -0.000232118, -0.038867, -0.0149739, ...]\n",
      "  T#111(retinanet/res4e_branch2b/Conv2D) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 112, data:[-0.0325724, -0.028471, -0.0664595, -0.0147888, -0.0203647, ...]\n",
      "  T#112(retinanet/res4e_branch2c/Conv2D) shape:[1024, 1, 1, 256], type:FLOAT32 RO 1048576 bytes, buffer: 113, data:[-0.0500581, 0.0927623, 0.0671487, -0.0183484, -0.0782414, ...]\n",
      "  T#113(retinanet/res4f_branch2a/Conv2D) shape:[256, 1, 1, 1024], type:FLOAT32 RO 1048576 bytes, buffer: 114, data:[-0.00971836, -0.0345225, -0.055255, -0.0634911, -0.00841653, ...]\n",
      "  T#114(retinanet/res4f_branch2b/Conv2D) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 115, data:[0.0571535, 0.0302676, -0.0114637, 0.0110959, 0.00266861, ...]\n",
      "  T#115(retinanet/res4f_branch2c/Conv2D) shape:[1024, 1, 1, 256], type:FLOAT32 RO 1048576 bytes, buffer: 116, data:[-0.0422973, -0.0445139, 0.000225431, -0.0422244, 0.0552635, ...]\n",
      "  T#116(retinanet/C4_reduced/Conv2D) shape:[256, 1, 1, 1024], type:FLOAT32 RO 1048576 bytes, buffer: 117, data:[0.0101784, 0.0225218, -0.0747209, 0.0292871, 0.0648674, ...]\n",
      "  T#117(retinanet/res5a_branch1/Conv2D) shape:[2048, 1, 1, 1024], type:FLOAT32 RO 8388608 bytes, buffer: 118, data:[-0.073911, 0.00453859, -0.0170966, 0.00156419, -0.0608638, ...]\n",
      "  T#118(retinanet/res5a_branch2a/Conv2D) shape:[512, 1, 1, 1024], type:FLOAT32 RO 2097152 bytes, buffer: 119, data:[0.00954459, -0.0134239, 0.0513214, 0.0221076, -0.0328828, ...]\n",
      "  T#119(retinanet/res5a_branch2b/Conv2D) shape:[512, 3, 3, 512], type:FLOAT32 RO 9437184 bytes, buffer: 120, data:[-0.000502251, 0.0219493, 0.0335113, 0.00835865, -0.00909237, ...]\n",
      "  T#120(retinanet/res5a_branch2c/Conv2D) shape:[2048, 1, 1, 512], type:FLOAT32 RO 4194304 bytes, buffer: 121, data:[-0.186932, -0.0556401, -0.196296, 0.260215, 1.07595, ...]\n",
      "  T#121(retinanet/res5b_branch2a/Conv2D) shape:[512, 1, 1, 2048], type:FLOAT32 RO 4194304 bytes, buffer: 122, data:[-0.00734602, 0.0101153, 0.0020222, -0.0118333, 0.00263452, ...]\n",
      "  T#122(retinanet/res5b_branch2b/Conv2D) shape:[512, 3, 3, 512], type:FLOAT32 RO 9437184 bytes, buffer: 123, data:[0.0211581, -0.0110105, -0.00689923, -0.00189334, -0.0382643, ...]\n",
      "  T#123(retinanet/res5b_branch2c/Conv2D) shape:[2048, 1, 1, 512], type:FLOAT32 RO 4194304 bytes, buffer: 124, data:[0.13271, -0.0059123, 0.423387, -0.0654457, -0.0593388, ...]\n",
      "  T#124(retinanet/res5c_branch2a/Conv2D) shape:[512, 1, 1, 2048], type:FLOAT32 RO 4194304 bytes, buffer: 125, data:[-0.0129665, 0.0118278, 0.00548595, -0.0110701, -0.0105735, ...]\n",
      "  T#125(retinanet/res5c_branch2b/Conv2D) shape:[512, 3, 3, 512], type:FLOAT32 RO 9437184 bytes, buffer: 126, data:[-0.043112, -0.000195644, 0.0114277, 0.024217, -0.0247556, ...]\n",
      "  T#126(retinanet/res5c_branch2c/Conv2D) shape:[2048, 1, 1, 512], type:FLOAT32 RO 4194304 bytes, buffer: 127, data:[0.243701, 0.0923204, 0.330575, -0.196351, -0.251383, ...]\n",
      "  T#127(retinanet/C5_reduced/Conv2D) shape:[256, 1, 1, 2048], type:FLOAT32 RO 2097152 bytes, buffer: 128, data:[0.0169095, -0.0346877, -0.00317066, 0.0537651, -0.0311659, ...]\n",
      "  T#128(retinanet/P5/Conv2D) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 129, data:[-0.0186785, 0.0234531, -0.0377679, -0.00568221, 0.0300737, ...]\n",
      "  T#129(retinanet/classification_submodel/pyramid_classification_0/Conv2D_3) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 130, data:[-0.00435792, -0.00140948, -0.00561434, 0.008287, 0.00186111, ...]\n",
      "  T#130(retinanet/classification_submodel/pyramid_classification_1/Conv2D_3) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 131, data:[-0.0146441, 0.0121931, 0.00339752, -0.000620402, -0.0182116, ...]\n",
      "  T#131(retinanet/classification_submodel/pyramid_classification_2/Conv2D_3) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 132, data:[-0.00389682, -0.0200403, -0.00117556, 0.0045091, -0.00586712, ...]\n",
      "  T#132(retinanet/classification_submodel/pyramid_classification_3/Conv2D_3) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 133, data:[-0.00256099, -0.0152822, 0.00483776, 0.00140023, 0.00738751, ...]\n",
      "  T#133(retinanet/classification_submodel/pyramid_classification/Conv2D_3) shape:[18, 3, 3, 256], type:FLOAT32 RO 165888 bytes, buffer: 134, data:[0.00322544, 0.00407088, 0.0053794, -0.0101322, 0.00447614, ...]\n",
      "  T#134(retinanet/regression_submodel/pyramid_regression_0/Conv2D_3) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 135, data:[-0.00611221, 0.00162253, 0.00201515, 0.00235672, -0.0351836, ...]\n",
      "  T#135(retinanet/regression_submodel/pyramid_regression_1/Conv2D_3) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 136, data:[-0.00854493, -0.000690476, 0.00439706, -0.00626267, -0.0323589, ...]\n",
      "  T#136(retinanet/regression_submodel/pyramid_regression_2/Conv2D_3) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 137, data:[-0.00649997, -0.000437556, 0.00802442, 0.000717214, -0.0307877, ...]\n",
      "  T#137(retinanet/regression_submodel/pyramid_regression_3/Conv2D_3) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 138, data:[-0.00668381, 7.67645e-06, 0.00488403, -0.00296285, -0.036433, ...]\n",
      "  T#138(retinanet/regression_submodel/pyramid_regression/Conv2D_3) shape:[36, 3, 3, 256], type:FLOAT32 RO 331776 bytes, buffer: 139, data:[-0.00263978, 0.00886245, -0.00675373, 0.00432915, -0.011396, ...]\n",
      "  T#139(retinanet/P4/Conv2D) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 140, data:[-0.00021335, -0.0287158, 0.0147342, 0.00591488, -0.0279551, ...]\n",
      "  T#140(retinanet/P3/Conv2D) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 141, data:[0.0295024, -0.0222418, -0.00631509, -0.0218435, 0.0110401, ...]\n",
      "  T#141(retinanet/P6/Conv2D) shape:[256, 3, 3, 2048], type:FLOAT32 RO 18874368 bytes, buffer: 142, data:[0.000595994, 0.0148811, 0.000333636, -0.00412403, 0.0143393, ...]\n",
      "  T#142(retinanet/P7/Conv2D) shape:[256, 3, 3, 256], type:FLOAT32 RO 2359296 bytes, buffer: 143, data:[0.0135432, -0.0192731, 0.0104433, 0.0165959, 0.0190638, ...]\n",
      "  T#143(retinanet/P4_upsampled/strided_slice/stack) shape:[1], type:INT32 RO 4 bytes, buffer: 144, data:[1]\n",
      "  T#144(retinanet/P4_upsampled/strided_slice/stack_1) shape:[1], type:INT32 RO 4 bytes, buffer: 145, data:[2]\n",
      "  T#145(retinanet/P4_upsampled/strided_slice_1/stack_1) shape:[1], type:INT32 RO 4 bytes, buffer: 146, data:[3]\n",
      "  T#146(retinanet/classification_submodel/pyramid_classification_reshape/Reshape/shape/1) shape:[], type:INT32 RO 4 bytes, buffer: 147, data:[-1]\n",
      "  T#147(retinanet/classification_submodel/pyramid_classification_reshape/Reshape/shape/2) shape:[], type:INT32 RO 4 bytes, buffer: 145, data:[2]\n",
      "  T#148(retinanet/classification_submodel/pyramid_classification_reshape/strided_slice/stack) shape:[1], type:INT32 RO 4 bytes, buffer: 149, data:[0]\n",
      "  T#149(retinanet/padding2a_branch2b/Pad/paddings) shape:[4, 2], type:INT32 RO 32 bytes, buffer: 150, data:[0, 0, 1, 1, 1, ...]\n",
      "  T#150(retinanet/regression_submodel/pyramid_regression_reshape/Reshape/shape/2) shape:[], type:INT32 RO 4 bytes, buffer: 151, data:[4]\n",
      "  T#151(retinanet/conv1_relu/Relu;retinanet/bn_conv1/FusedBatchNormV3;retinanet/res2c_branch2b/Conv2D;retinanet/conv1/Conv2D) shape_signature:[-1, -1, -1, 64], type:FLOAT32\n",
      "  T#152(retinanet/pool1/MaxPool) shape_signature:[-1, -1, -1, 64], type:FLOAT32\n",
      "  T#153(retinanet/bn2a_branch1/FusedBatchNormV3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/res2a_branch1/Conv2D) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#154(retinanet/res2a_branch2a_relu/Relu;retinanet/bn2a_branch2a/FusedBatchNormV3;retinanet/res2c_branch2b/Conv2D;retinanet/res2a_branch2a/Conv2D) shape_signature:[-1, -1, -1, 64], type:FLOAT32\n",
      "  T#155(retinanet/padding2a_branch2b/Pad) shape_signature:[-1, -1, -1, 64], type:FLOAT32\n",
      "  T#156(retinanet/res2a_branch2b_relu/Relu;retinanet/bn2a_branch2b/FusedBatchNormV3;retinanet/res2c_branch2b/Conv2D;retinanet/res2a_branch2b/Conv2D) shape_signature:[-1, -1, -1, 64], type:FLOAT32\n",
      "  T#157(retinanet/bn2a_branch2c/FusedBatchNormV3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/res2a_branch2c/Conv2D) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#158(retinanet/res2a_relu/Relu;retinanet/res2a/add) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#159(retinanet/res2b_branch2a_relu/Relu;retinanet/bn2b_branch2a/FusedBatchNormV3;retinanet/res2c_branch2b/Conv2D;retinanet/res2b_branch2a/Conv2D) shape_signature:[-1, -1, -1, 64], type:FLOAT32\n",
      "  T#160(retinanet/padding2b_branch2b/Pad) shape_signature:[-1, -1, -1, 64], type:FLOAT32\n",
      "  T#161(retinanet/res2b_branch2b_relu/Relu;retinanet/bn2b_branch2b/FusedBatchNormV3;retinanet/res2c_branch2b/Conv2D;retinanet/res2b_branch2b/Conv2D) shape_signature:[-1, -1, -1, 64], type:FLOAT32\n",
      "  T#162(retinanet/bn2b_branch2c/FusedBatchNormV3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/res2b_branch2c/Conv2D) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#163(retinanet/res2b_relu/Relu;retinanet/res2b/add) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#164(retinanet/res2c_branch2a_relu/Relu;retinanet/bn2c_branch2a/FusedBatchNormV3;retinanet/res2c_branch2b/Conv2D;retinanet/res2c_branch2a/Conv2D) shape_signature:[-1, -1, -1, 64], type:FLOAT32\n",
      "  T#165(retinanet/padding2c_branch2b/Pad) shape_signature:[-1, -1, -1, 64], type:FLOAT32\n",
      "  T#166(retinanet/res2c_branch2b_relu/Relu;retinanet/bn2c_branch2b/FusedBatchNormV3;retinanet/res2c_branch2b/Conv2D) shape_signature:[-1, -1, -1, 64], type:FLOAT32\n",
      "  T#167(retinanet/bn2c_branch2c/FusedBatchNormV3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/res2c_branch2c/Conv2D) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#168(retinanet/res2c_relu/Relu;retinanet/res2c/add) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#169(retinanet/bn3a_branch1/FusedBatchNormV3;retinanet/res5c_branch2b/Conv2D;retinanet/res3a_branch1/Conv2D) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#170(retinanet/res3a_branch2a_relu/Relu;retinanet/bn3a_branch2a/FusedBatchNormV3;retinanet/res3d_branch2b/Conv2D;retinanet/res3a_branch2a/Conv2D) shape_signature:[-1, -1, -1, 128], type:FLOAT32\n",
      "  T#171(retinanet/padding3a_branch2b/Pad) shape_signature:[-1, -1, -1, 128], type:FLOAT32\n",
      "  T#172(retinanet/res3a_branch2b_relu/Relu;retinanet/bn3a_branch2b/FusedBatchNormV3;retinanet/res3d_branch2b/Conv2D;retinanet/res3a_branch2b/Conv2D) shape_signature:[-1, -1, -1, 128], type:FLOAT32\n",
      "  T#173(retinanet/bn3a_branch2c/FusedBatchNormV3;retinanet/res5c_branch2b/Conv2D;retinanet/res3a_branch2c/Conv2D) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#174(retinanet/res3a_relu/Relu;retinanet/res3a/add) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#175(retinanet/res3b_branch2a_relu/Relu;retinanet/bn3b_branch2a/FusedBatchNormV3;retinanet/res3d_branch2b/Conv2D;retinanet/res3b_branch2a/Conv2D) shape_signature:[-1, -1, -1, 128], type:FLOAT32\n",
      "  T#176(retinanet/padding3b_branch2b/Pad) shape_signature:[-1, -1, -1, 128], type:FLOAT32\n",
      "  T#177(retinanet/res3b_branch2b_relu/Relu;retinanet/bn3b_branch2b/FusedBatchNormV3;retinanet/res3d_branch2b/Conv2D;retinanet/res3b_branch2b/Conv2D) shape_signature:[-1, -1, -1, 128], type:FLOAT32\n",
      "  T#178(retinanet/bn3b_branch2c/FusedBatchNormV3;retinanet/res5c_branch2b/Conv2D;retinanet/res3b_branch2c/Conv2D) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#179(retinanet/res3b_relu/Relu;retinanet/res3b/add) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#180(retinanet/res3c_branch2a_relu/Relu;retinanet/bn3c_branch2a/FusedBatchNormV3;retinanet/res3d_branch2b/Conv2D;retinanet/res3c_branch2a/Conv2D) shape_signature:[-1, -1, -1, 128], type:FLOAT32\n",
      "  T#181(retinanet/padding3c_branch2b/Pad) shape_signature:[-1, -1, -1, 128], type:FLOAT32\n",
      "  T#182(retinanet/res3c_branch2b_relu/Relu;retinanet/bn3c_branch2b/FusedBatchNormV3;retinanet/res3d_branch2b/Conv2D;retinanet/res3c_branch2b/Conv2D) shape_signature:[-1, -1, -1, 128], type:FLOAT32\n",
      "  T#183(retinanet/bn3c_branch2c/FusedBatchNormV3;retinanet/res5c_branch2b/Conv2D;retinanet/res3c_branch2c/Conv2D) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#184(retinanet/res3c_relu/Relu;retinanet/res3c/add) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#185(retinanet/res3d_branch2a_relu/Relu;retinanet/bn3d_branch2a/FusedBatchNormV3;retinanet/res3d_branch2b/Conv2D;retinanet/res3d_branch2a/Conv2D) shape_signature:[-1, -1, -1, 128], type:FLOAT32\n",
      "  T#186(retinanet/padding3d_branch2b/Pad) shape_signature:[-1, -1, -1, 128], type:FLOAT32\n",
      "  T#187(retinanet/res3d_branch2b_relu/Relu;retinanet/bn3d_branch2b/FusedBatchNormV3;retinanet/res3d_branch2b/Conv2D) shape_signature:[-1, -1, -1, 128], type:FLOAT32\n",
      "  T#188(retinanet/bn3d_branch2c/FusedBatchNormV3;retinanet/res5c_branch2b/Conv2D;retinanet/res3d_branch2c/Conv2D) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#189(retinanet/res3d_relu/Relu;retinanet/res3d/add) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#190(retinanet/C3_reduced/BiasAdd;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/C3_reduced/Conv2D;retinanet/C3_reduced/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#191(retinanet/P4_upsampled/Shape) shape:[4], type:INT32\n",
      "  T#192(retinanet/P4_upsampled/strided_slice) shape:[], type:INT32\n",
      "  T#193(retinanet/P4_upsampled/strided_slice_1) shape:[], type:INT32\n",
      "  T#194(retinanet/P4_upsampled/resize/size) shape:[2], type:INT32\n",
      "  T#195(retinanet/bn4a_branch1/FusedBatchNormV3;retinanet/res4f_branch2c/Conv2D;retinanet/res4a_branch1/Conv2D) shape_signature:[-1, -1, -1, 1024], type:FLOAT32\n",
      "  T#196(retinanet/res4a_branch2a_relu/Relu;retinanet/bn4a_branch2a/FusedBatchNormV3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/res4a_branch2a/Conv2D) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#197(retinanet/padding4a_branch2b/Pad) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#198(retinanet/res4a_branch2b_relu/Relu;retinanet/bn4a_branch2b/FusedBatchNormV3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/res4a_branch2b/Conv2D) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#199(retinanet/bn4a_branch2c/FusedBatchNormV3;retinanet/res4f_branch2c/Conv2D;retinanet/res4a_branch2c/Conv2D) shape_signature:[-1, -1, -1, 1024], type:FLOAT32\n",
      "  T#200(retinanet/res4a_relu/Relu;retinanet/res4a/add) shape_signature:[-1, -1, -1, 1024], type:FLOAT32\n",
      "  T#201(retinanet/res4b_branch2a_relu/Relu;retinanet/bn4b_branch2a/FusedBatchNormV3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/res4b_branch2a/Conv2D) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#202(retinanet/padding4b_branch2b/Pad) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#203(retinanet/res4b_branch2b_relu/Relu;retinanet/bn4b_branch2b/FusedBatchNormV3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/res4b_branch2b/Conv2D) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#204(retinanet/bn4b_branch2c/FusedBatchNormV3;retinanet/res4f_branch2c/Conv2D;retinanet/res4b_branch2c/Conv2D) shape_signature:[-1, -1, -1, 1024], type:FLOAT32\n",
      "  T#205(retinanet/res4b_relu/Relu;retinanet/res4b/add) shape_signature:[-1, -1, -1, 1024], type:FLOAT32\n",
      "  T#206(retinanet/res4c_branch2a_relu/Relu;retinanet/bn4c_branch2a/FusedBatchNormV3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/res4c_branch2a/Conv2D) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#207(retinanet/padding4c_branch2b/Pad) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#208(retinanet/res4c_branch2b_relu/Relu;retinanet/bn4c_branch2b/FusedBatchNormV3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/res4c_branch2b/Conv2D) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#209(retinanet/bn4c_branch2c/FusedBatchNormV3;retinanet/res4f_branch2c/Conv2D;retinanet/res4c_branch2c/Conv2D) shape_signature:[-1, -1, -1, 1024], type:FLOAT32\n",
      "  T#210(retinanet/res4c_relu/Relu;retinanet/res4c/add) shape_signature:[-1, -1, -1, 1024], type:FLOAT32\n",
      "  T#211(retinanet/res4d_branch2a_relu/Relu;retinanet/bn4d_branch2a/FusedBatchNormV3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/res4d_branch2a/Conv2D) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#212(retinanet/padding4d_branch2b/Pad) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#213(retinanet/res4d_branch2b_relu/Relu;retinanet/bn4d_branch2b/FusedBatchNormV3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/res4d_branch2b/Conv2D) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#214(retinanet/bn4d_branch2c/FusedBatchNormV3;retinanet/res4f_branch2c/Conv2D;retinanet/res4d_branch2c/Conv2D) shape_signature:[-1, -1, -1, 1024], type:FLOAT32\n",
      "  T#215(retinanet/res4d_relu/Relu;retinanet/res4d/add) shape_signature:[-1, -1, -1, 1024], type:FLOAT32\n",
      "  T#216(retinanet/res4e_branch2a_relu/Relu;retinanet/bn4e_branch2a/FusedBatchNormV3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/res4e_branch2a/Conv2D) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#217(retinanet/padding4e_branch2b/Pad) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#218(retinanet/res4e_branch2b_relu/Relu;retinanet/bn4e_branch2b/FusedBatchNormV3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/res4e_branch2b/Conv2D) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#219(retinanet/bn4e_branch2c/FusedBatchNormV3;retinanet/res4f_branch2c/Conv2D;retinanet/res4e_branch2c/Conv2D) shape_signature:[-1, -1, -1, 1024], type:FLOAT32\n",
      "  T#220(retinanet/res4e_relu/Relu;retinanet/res4e/add) shape_signature:[-1, -1, -1, 1024], type:FLOAT32\n",
      "  T#221(retinanet/res4f_branch2a_relu/Relu;retinanet/bn4f_branch2a/FusedBatchNormV3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/res4f_branch2a/Conv2D) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#222(retinanet/padding4f_branch2b/Pad) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#223(retinanet/res4f_branch2b_relu/Relu;retinanet/bn4f_branch2b/FusedBatchNormV3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/res4f_branch2b/Conv2D) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#224(retinanet/bn4f_branch2c/FusedBatchNormV3;retinanet/res4f_branch2c/Conv2D) shape_signature:[-1, -1, -1, 1024], type:FLOAT32\n",
      "  T#225(retinanet/res4f_relu/Relu;retinanet/res4f/add) shape_signature:[-1, -1, -1, 1024], type:FLOAT32\n",
      "  T#226(retinanet/C4_reduced/BiasAdd;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/C4_reduced/Conv2D;retinanet/C4_reduced/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#227(retinanet/P5_upsampled/Shape) shape:[4], type:INT32\n",
      "  T#228(retinanet/P5_upsampled/strided_slice) shape:[], type:INT32\n",
      "  T#229(retinanet/P5_upsampled/strided_slice_1) shape:[], type:INT32\n",
      "  T#230(retinanet/P5_upsampled/resize/size) shape:[2], type:INT32\n",
      "  T#231(retinanet/bn5a_branch1/FusedBatchNormV3;retinanet/res5c_branch2c/Conv2D;retinanet/res5a_branch1/Conv2D) shape_signature:[-1, -1, -1, 2048], type:FLOAT32\n",
      "  T#232(retinanet/res5a_branch2a_relu/Relu;retinanet/bn5a_branch2a/FusedBatchNormV3;retinanet/res5c_branch2b/Conv2D;retinanet/res5a_branch2a/Conv2D) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#233(retinanet/padding5a_branch2b/Pad) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#234(retinanet/res5a_branch2b_relu/Relu;retinanet/bn5a_branch2b/FusedBatchNormV3;retinanet/res5c_branch2b/Conv2D;retinanet/res5a_branch2b/Conv2D) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#235(retinanet/bn5a_branch2c/FusedBatchNormV3;retinanet/res5c_branch2c/Conv2D;retinanet/res5a_branch2c/Conv2D) shape_signature:[-1, -1, -1, 2048], type:FLOAT32\n",
      "  T#236(retinanet/res5a_relu/Relu;retinanet/res5a/add) shape_signature:[-1, -1, -1, 2048], type:FLOAT32\n",
      "  T#237(retinanet/res5b_branch2a_relu/Relu;retinanet/bn5b_branch2a/FusedBatchNormV3;retinanet/res5c_branch2b/Conv2D;retinanet/res5b_branch2a/Conv2D) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#238(retinanet/padding5b_branch2b/Pad) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#239(retinanet/res5b_branch2b_relu/Relu;retinanet/bn5b_branch2b/FusedBatchNormV3;retinanet/res5c_branch2b/Conv2D;retinanet/res5b_branch2b/Conv2D) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#240(retinanet/bn5b_branch2c/FusedBatchNormV3;retinanet/res5c_branch2c/Conv2D;retinanet/res5b_branch2c/Conv2D) shape_signature:[-1, -1, -1, 2048], type:FLOAT32\n",
      "  T#241(retinanet/res5b_relu/Relu;retinanet/res5b/add) shape_signature:[-1, -1, -1, 2048], type:FLOAT32\n",
      "  T#242(retinanet/res5c_branch2a_relu/Relu;retinanet/bn5c_branch2a/FusedBatchNormV3;retinanet/res5c_branch2b/Conv2D;retinanet/res5c_branch2a/Conv2D) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#243(retinanet/padding5c_branch2b/Pad) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#244(retinanet/res5c_branch2b_relu/Relu;retinanet/bn5c_branch2b/FusedBatchNormV3;retinanet/res5c_branch2b/Conv2D) shape_signature:[-1, -1, -1, 512], type:FLOAT32\n",
      "  T#245(retinanet/bn5c_branch2c/FusedBatchNormV3;retinanet/res5c_branch2c/Conv2D) shape_signature:[-1, -1, -1, 2048], type:FLOAT32\n",
      "  T#246(retinanet/res5c_relu/Relu;retinanet/res5c/add) shape_signature:[-1, -1, -1, 2048], type:FLOAT32\n",
      "  T#247(retinanet/C5_reduced/BiasAdd;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/C5_reduced/Conv2D;retinanet/C5_reduced/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#248(retinanet/P5/BiasAdd;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/P5/Conv2D;retinanet/P5/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#249(retinanet/classification_submodel/pyramid_classification_0/Relu_2;retinanet/classification_submodel/pyramid_classification_0/BiasAdd_2;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_0/Conv2D_2;retinanet/classification_submodel/pyramid_classification_0/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#250(retinanet/classification_submodel/pyramid_classification_1/Relu_2;retinanet/classification_submodel/pyramid_classification_1/BiasAdd_2;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_1/Conv2D_2;retinanet/classification_submodel/pyramid_classification_1/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#251(retinanet/classification_submodel/pyramid_classification_2/Relu_2;retinanet/classification_submodel/pyramid_classification_2/BiasAdd_2;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_2/Conv2D_2;retinanet/classification_submodel/pyramid_classification_2/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#252(retinanet/classification_submodel/pyramid_classification_3/Relu_2;retinanet/classification_submodel/pyramid_classification_3/BiasAdd_2;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_3/Conv2D_2;retinanet/classification_submodel/pyramid_classification_3/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#253(retinanet/classification_submodel/pyramid_classification/BiasAdd_2;retinanet/classification_submodel/pyramid_classification/Conv2D_3;retinanet/classification_submodel/pyramid_classification/Conv2D_2;retinanet/classification_submodel/pyramid_classification/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 18], type:FLOAT32\n",
      "  T#254(retinanet/classification_submodel/pyramid_classification_reshape/Shape_2) shape:[4], type:INT32\n",
      "  T#255(retinanet/classification_submodel/pyramid_classification_reshape/strided_slice_2) shape:[], type:INT32\n",
      "  T#256(retinanet/classification_submodel/pyramid_classification_reshape/Reshape_2/shape) shape:[3], type:INT32\n",
      "  T#257(retinanet/classification_submodel/pyramid_classification_sigmoid/Sigmoid_2;retinanet/classification_submodel/pyramid_classification_reshape/Reshape_2) shape_signature:[-1, -1, -1, 18], type:FLOAT32\n",
      "  T#258(retinanet/classification_submodel/pyramid_classification_sigmoid/Sigmoid_2;retinanet/classification_submodel/pyramid_classification_reshape/Reshape_21) shape_signature:[-1, -1, 2], type:FLOAT32\n",
      "  T#259(retinanet/regression_submodel/pyramid_regression_0/Relu_2;retinanet/regression_submodel/pyramid_regression_0/BiasAdd_2;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_0/Conv2D_2;retinanet/regression_submodel/pyramid_regression_0/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#260(retinanet/regression_submodel/pyramid_regression_1/Relu_2;retinanet/regression_submodel/pyramid_regression_1/BiasAdd_2;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_1/Conv2D_2;retinanet/regression_submodel/pyramid_regression_1/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#261(retinanet/regression_submodel/pyramid_regression_2/Relu_2;retinanet/regression_submodel/pyramid_regression_2/BiasAdd_2;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_2/Conv2D_2;retinanet/regression_submodel/pyramid_regression_2/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#262(retinanet/regression_submodel/pyramid_regression_3/Relu_2;retinanet/regression_submodel/pyramid_regression_3/BiasAdd_2;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_2;retinanet/regression_submodel/pyramid_regression_3/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#263(retinanet/regression_submodel/pyramid_regression/BiasAdd_2;retinanet/regression_submodel/pyramid_regression/Conv2D_3;retinanet/regression_submodel/pyramid_regression/Conv2D_2;retinanet/regression_submodel/pyramid_regression/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 36], type:FLOAT32\n",
      "  T#264(retinanet/regression_submodel/pyramid_regression_reshape/Shape_2) shape:[4], type:INT32\n",
      "  T#265(retinanet/regression_submodel/pyramid_regression_reshape/strided_slice_2) shape:[], type:INT32\n",
      "  T#266(retinanet/regression_submodel/pyramid_regression_reshape/Reshape_2/shape) shape:[3], type:INT32\n",
      "  T#267(retinanet/regression_submodel/pyramid_regression_reshape/Reshape_2) shape_signature:[-1, -1, 4], type:FLOAT32\n",
      "  T#268(retinanet/P5_upsampled/resize/ResizeNearestNeighbor) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#269(retinanet/P4_merged/add) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#270(retinanet/P4/BiasAdd;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/P4/Conv2D;retinanet/P4/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#271(retinanet/classification_submodel/pyramid_classification_0/Relu_1;retinanet/classification_submodel/pyramid_classification_0/BiasAdd_1;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_0/Conv2D_1;retinanet/classification_submodel/pyramid_classification_0/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#272(retinanet/classification_submodel/pyramid_classification_1/Relu_1;retinanet/classification_submodel/pyramid_classification_1/BiasAdd_1;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_1/Conv2D_1;retinanet/classification_submodel/pyramid_classification_1/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#273(retinanet/classification_submodel/pyramid_classification_2/Relu_1;retinanet/classification_submodel/pyramid_classification_2/BiasAdd_1;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_2/Conv2D_1;retinanet/classification_submodel/pyramid_classification_2/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#274(retinanet/classification_submodel/pyramid_classification_3/Relu_1;retinanet/classification_submodel/pyramid_classification_3/BiasAdd_1;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_3/Conv2D_1;retinanet/classification_submodel/pyramid_classification_3/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#275(retinanet/classification_submodel/pyramid_classification/BiasAdd_1;retinanet/classification_submodel/pyramid_classification/Conv2D_3;retinanet/classification_submodel/pyramid_classification/Conv2D_1;retinanet/classification_submodel/pyramid_classification/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 18], type:FLOAT32\n",
      "  T#276(retinanet/classification_submodel/pyramid_classification_reshape/Shape_1) shape:[4], type:INT32\n",
      "  T#277(retinanet/classification_submodel/pyramid_classification_reshape/strided_slice_1) shape:[], type:INT32\n",
      "  T#278(retinanet/classification_submodel/pyramid_classification_reshape/Reshape_1/shape) shape:[3], type:INT32\n",
      "  T#279(retinanet/classification_submodel/pyramid_classification_sigmoid/Sigmoid_1;retinanet/classification_submodel/pyramid_classification_reshape/Reshape_1) shape_signature:[-1, -1, -1, 18], type:FLOAT32\n",
      "  T#280(retinanet/classification_submodel/pyramid_classification_sigmoid/Sigmoid_1;retinanet/classification_submodel/pyramid_classification_reshape/Reshape_11) shape_signature:[-1, -1, 2], type:FLOAT32\n",
      "  T#281(retinanet/regression_submodel/pyramid_regression_0/Relu_1;retinanet/regression_submodel/pyramid_regression_0/BiasAdd_1;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_0/Conv2D_1;retinanet/regression_submodel/pyramid_regression_0/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#282(retinanet/regression_submodel/pyramid_regression_1/Relu_1;retinanet/regression_submodel/pyramid_regression_1/BiasAdd_1;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_1/Conv2D_1;retinanet/regression_submodel/pyramid_regression_1/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#283(retinanet/regression_submodel/pyramid_regression_2/Relu_1;retinanet/regression_submodel/pyramid_regression_2/BiasAdd_1;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_2/Conv2D_1;retinanet/regression_submodel/pyramid_regression_2/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#284(retinanet/regression_submodel/pyramid_regression_3/Relu_1;retinanet/regression_submodel/pyramid_regression_3/BiasAdd_1;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_1;retinanet/regression_submodel/pyramid_regression_3/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#285(retinanet/regression_submodel/pyramid_regression/BiasAdd_1;retinanet/regression_submodel/pyramid_regression/Conv2D_3;retinanet/regression_submodel/pyramid_regression/Conv2D_1;retinanet/regression_submodel/pyramid_regression/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 36], type:FLOAT32\n",
      "  T#286(retinanet/regression_submodel/pyramid_regression_reshape/Shape_1) shape:[4], type:INT32\n",
      "  T#287(retinanet/regression_submodel/pyramid_regression_reshape/strided_slice_1) shape:[], type:INT32\n",
      "  T#288(retinanet/regression_submodel/pyramid_regression_reshape/Reshape_1/shape) shape:[3], type:INT32\n",
      "  T#289(retinanet/regression_submodel/pyramid_regression_reshape/Reshape_1) shape_signature:[-1, -1, 4], type:FLOAT32\n",
      "  T#290(retinanet/P4_upsampled/resize/ResizeNearestNeighbor) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#291(retinanet/P3_merged/add) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#292(retinanet/P3/BiasAdd;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/P3/Conv2D;retinanet/P3/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#293(retinanet/classification_submodel/pyramid_classification_0/Relu;retinanet/classification_submodel/pyramid_classification_0/BiasAdd;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_0/Conv2D;retinanet/classification_submodel/pyramid_classification_0/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#294(retinanet/classification_submodel/pyramid_classification_1/Relu;retinanet/classification_submodel/pyramid_classification_1/BiasAdd;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_1/Conv2D;retinanet/classification_submodel/pyramid_classification_1/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#295(retinanet/classification_submodel/pyramid_classification_2/Relu;retinanet/classification_submodel/pyramid_classification_2/BiasAdd;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_2/Conv2D;retinanet/classification_submodel/pyramid_classification_2/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#296(retinanet/classification_submodel/pyramid_classification_3/Relu;retinanet/classification_submodel/pyramid_classification_3/BiasAdd;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_3/Conv2D;retinanet/classification_submodel/pyramid_classification_3/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#297(retinanet/classification_submodel/pyramid_classification/BiasAdd;retinanet/classification_submodel/pyramid_classification/Conv2D_3;retinanet/classification_submodel/pyramid_classification/Conv2D;retinanet/classification_submodel/pyramid_classification/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 18], type:FLOAT32\n",
      "  T#298(retinanet/classification_submodel/pyramid_classification_reshape/Shape) shape:[4], type:INT32\n",
      "  T#299(retinanet/classification_submodel/pyramid_classification_reshape/strided_slice) shape:[], type:INT32\n",
      "  T#300(retinanet/classification_submodel/pyramid_classification_reshape/Reshape/shape) shape:[3], type:INT32\n",
      "  T#301(retinanet/classification_submodel/pyramid_classification_sigmoid/Sigmoid;retinanet/classification_submodel/pyramid_classification_reshape/Reshape) shape_signature:[-1, -1, -1, 18], type:FLOAT32\n",
      "  T#302(retinanet/classification_submodel/pyramid_classification_sigmoid/Sigmoid;retinanet/classification_submodel/pyramid_classification_reshape/Reshape1) shape_signature:[-1, -1, 2], type:FLOAT32\n",
      "  T#303(retinanet/regression_submodel/pyramid_regression_0/Relu;retinanet/regression_submodel/pyramid_regression_0/BiasAdd;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_0/Conv2D;retinanet/regression_submodel/pyramid_regression_0/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#304(retinanet/regression_submodel/pyramid_regression_1/Relu;retinanet/regression_submodel/pyramid_regression_1/BiasAdd;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_1/Conv2D;retinanet/regression_submodel/pyramid_regression_1/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#305(retinanet/regression_submodel/pyramid_regression_2/Relu;retinanet/regression_submodel/pyramid_regression_2/BiasAdd;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_2/Conv2D;retinanet/regression_submodel/pyramid_regression_2/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#306(retinanet/regression_submodel/pyramid_regression_3/Relu;retinanet/regression_submodel/pyramid_regression_3/BiasAdd;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_3/Conv2D;retinanet/regression_submodel/pyramid_regression_3/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#307(retinanet/regression_submodel/pyramid_regression/BiasAdd;retinanet/regression_submodel/pyramid_regression/Conv2D_3;retinanet/regression_submodel/pyramid_regression/Conv2D;retinanet/regression_submodel/pyramid_regression/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 36], type:FLOAT32\n",
      "  T#308(retinanet/regression_submodel/pyramid_regression_reshape/Shape) shape:[4], type:INT32\n",
      "  T#309(retinanet/regression_submodel/pyramid_regression_reshape/strided_slice) shape:[], type:INT32\n",
      "  T#310(retinanet/regression_submodel/pyramid_regression_reshape/Reshape/shape) shape:[3], type:INT32\n",
      "  T#311(retinanet/regression_submodel/pyramid_regression_reshape/Reshape) shape_signature:[-1, -1, 4], type:FLOAT32\n",
      "  T#312(retinanet/P6/BiasAdd;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/P6/Conv2D;retinanet/P6/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#313(retinanet/C6_relu/Relu) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#314(retinanet/P7/BiasAdd;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/P7/Conv2D;retinanet/P7/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#315(retinanet/classification_submodel/pyramid_classification_0/Relu_4;retinanet/classification_submodel/pyramid_classification_0/BiasAdd_4;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_0/Conv2D_4;retinanet/classification_submodel/pyramid_classification_0/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#316(retinanet/classification_submodel/pyramid_classification_1/Relu_4;retinanet/classification_submodel/pyramid_classification_1/BiasAdd_4;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_1/Conv2D_4;retinanet/classification_submodel/pyramid_classification_1/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#317(retinanet/classification_submodel/pyramid_classification_2/Relu_4;retinanet/classification_submodel/pyramid_classification_2/BiasAdd_4;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_2/Conv2D_4;retinanet/classification_submodel/pyramid_classification_2/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#318(retinanet/classification_submodel/pyramid_classification_3/Relu_4;retinanet/classification_submodel/pyramid_classification_3/BiasAdd_4;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_3/Conv2D_4;retinanet/classification_submodel/pyramid_classification_3/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#319(retinanet/classification_submodel/pyramid_classification/BiasAdd_4;retinanet/classification_submodel/pyramid_classification/Conv2D_3;retinanet/classification_submodel/pyramid_classification/Conv2D_4;retinanet/classification_submodel/pyramid_classification/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 18], type:FLOAT32\n",
      "  T#320(retinanet/classification_submodel/pyramid_classification_reshape/Shape_4) shape:[4], type:INT32\n",
      "  T#321(retinanet/classification_submodel/pyramid_classification_reshape/strided_slice_4) shape:[], type:INT32\n",
      "  T#322(retinanet/classification_submodel/pyramid_classification_reshape/Reshape_4/shape) shape:[3], type:INT32\n",
      "  T#323(retinanet/classification_submodel/pyramid_classification_sigmoid/Sigmoid_4;retinanet/classification_submodel/pyramid_classification_reshape/Reshape_4) shape_signature:[-1, -1, -1, 18], type:FLOAT32\n",
      "  T#324(retinanet/classification_submodel/pyramid_classification_sigmoid/Sigmoid_4;retinanet/classification_submodel/pyramid_classification_reshape/Reshape_41) shape_signature:[-1, -1, 2], type:FLOAT32\n",
      "  T#325(retinanet/regression_submodel/pyramid_regression_0/Relu_4;retinanet/regression_submodel/pyramid_regression_0/BiasAdd_4;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_0/Conv2D_4;retinanet/regression_submodel/pyramid_regression_0/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#326(retinanet/regression_submodel/pyramid_regression_1/Relu_4;retinanet/regression_submodel/pyramid_regression_1/BiasAdd_4;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_1/Conv2D_4;retinanet/regression_submodel/pyramid_regression_1/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#327(retinanet/regression_submodel/pyramid_regression_2/Relu_4;retinanet/regression_submodel/pyramid_regression_2/BiasAdd_4;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_2/Conv2D_4;retinanet/regression_submodel/pyramid_regression_2/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#328(retinanet/regression_submodel/pyramid_regression_3/Relu_4;retinanet/regression_submodel/pyramid_regression_3/BiasAdd_4;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_4;retinanet/regression_submodel/pyramid_regression_3/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#329(retinanet/regression_submodel/pyramid_regression/BiasAdd_4;retinanet/regression_submodel/pyramid_regression/Conv2D_3;retinanet/regression_submodel/pyramid_regression/Conv2D_4;retinanet/regression_submodel/pyramid_regression/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 36], type:FLOAT32\n",
      "  T#330(retinanet/regression_submodel/pyramid_regression_reshape/Shape_4) shape:[4], type:INT32\n",
      "  T#331(retinanet/regression_submodel/pyramid_regression_reshape/strided_slice_4) shape:[], type:INT32\n",
      "  T#332(retinanet/regression_submodel/pyramid_regression_reshape/Reshape_4/shape) shape:[3], type:INT32\n",
      "  T#333(retinanet/regression_submodel/pyramid_regression_reshape/Reshape_4) shape_signature:[-1, -1, 4], type:FLOAT32\n",
      "  T#334(retinanet/classification_submodel/pyramid_classification_0/Relu_3;retinanet/classification_submodel/pyramid_classification_0/BiasAdd_3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_0/Conv2D_3;retinanet/classification_submodel/pyramid_classification_0/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#335(retinanet/classification_submodel/pyramid_classification_1/Relu_3;retinanet/classification_submodel/pyramid_classification_1/BiasAdd_3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_1/Conv2D_3;retinanet/classification_submodel/pyramid_classification_1/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#336(retinanet/classification_submodel/pyramid_classification_2/Relu_3;retinanet/classification_submodel/pyramid_classification_2/BiasAdd_3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_2/Conv2D_3;retinanet/classification_submodel/pyramid_classification_2/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#337(retinanet/classification_submodel/pyramid_classification_3/Relu_3;retinanet/classification_submodel/pyramid_classification_3/BiasAdd_3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_3/Conv2D_3;retinanet/classification_submodel/pyramid_classification_3/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#338(retinanet/classification_submodel/pyramid_classification/BiasAdd_3;retinanet/classification_submodel/pyramid_classification/Conv2D_3;retinanet/classification_submodel/pyramid_classification/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 18], type:FLOAT32\n",
      "  T#339(retinanet/classification_submodel/pyramid_classification_reshape/Shape_3) shape:[4], type:INT32\n",
      "  T#340(retinanet/classification_submodel/pyramid_classification_reshape/strided_slice_3) shape:[], type:INT32\n",
      "  T#341(retinanet/classification_submodel/pyramid_classification_reshape/Reshape_3/shape) shape:[3], type:INT32\n",
      "  T#342(retinanet/classification_submodel/pyramid_classification_sigmoid/Sigmoid_3;retinanet/classification_submodel/pyramid_classification_reshape/Reshape_3) shape_signature:[-1, -1, -1, 18], type:FLOAT32\n",
      "  T#343(retinanet/classification_submodel/pyramid_classification_sigmoid/Sigmoid_3;retinanet/classification_submodel/pyramid_classification_reshape/Reshape_31) shape_signature:[-1, -1, 2], type:FLOAT32\n",
      "  T#344(StatefulPartitionedCall:0) shape_signature:[-1, -1, 2], type:FLOAT32\n",
      "  T#345(retinanet/regression_submodel/pyramid_regression_0/Relu_3;retinanet/regression_submodel/pyramid_regression_0/BiasAdd_3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_0/Conv2D_3;retinanet/regression_submodel/pyramid_regression_0/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#346(retinanet/regression_submodel/pyramid_regression_1/Relu_3;retinanet/regression_submodel/pyramid_regression_1/BiasAdd_3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_1/Conv2D_3;retinanet/regression_submodel/pyramid_regression_1/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#347(retinanet/regression_submodel/pyramid_regression_2/Relu_3;retinanet/regression_submodel/pyramid_regression_2/BiasAdd_3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_2/Conv2D_3;retinanet/regression_submodel/pyramid_regression_2/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#348(retinanet/regression_submodel/pyramid_regression_3/Relu_3;retinanet/regression_submodel/pyramid_regression_3/BiasAdd_3;retinanet/regression_submodel/pyramid_regression_3/Conv2D_3;retinanet/regression_submodel/pyramid_regression_3/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 256], type:FLOAT32\n",
      "  T#349(retinanet/regression_submodel/pyramid_regression/BiasAdd_3;retinanet/regression_submodel/pyramid_regression/Conv2D_3;retinanet/regression_submodel/pyramid_regression/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, -1, 36], type:FLOAT32\n",
      "  T#350(retinanet/regression_submodel/pyramid_regression_reshape/Shape_3) shape:[4], type:INT32\n",
      "  T#351(retinanet/regression_submodel/pyramid_regression_reshape/strided_slice_3) shape:[], type:INT32\n",
      "  T#352(retinanet/regression_submodel/pyramid_regression_reshape/Reshape_3/shape) shape:[3], type:INT32\n",
      "  T#353(retinanet/regression_submodel/pyramid_regression_reshape/Reshape_3) shape_signature:[-1, -1, 4], type:FLOAT32\n",
      "  T#354(StatefulPartitionedCall:1) shape_signature:[-1, -1, 4], type:FLOAT32\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Your TFLite model has '1' signature_def(s).\n",
      "\n",
      "Signature#0 key: 'serving_default'\n",
      "- Subgraph: Subgraph#0\n",
      "- Inputs: \n",
      "    'input_1' : T#0\n",
      "- Outputs: \n",
      "    'classification' : T#344\n",
      "    'regression' : T#354\n",
      "\n",
      "---------------------------------------------------------------\n",
      "              Model size:  145372196 bytes\n",
      "    Non-data buffer size:      75952 bytes (00.05 %)\n",
      "  Total data buffer size:  145296244 bytes (99.95 %)\n",
      "    (Zero value buffers):          4 bytes (00.00 %)\n",
      "\n",
      "* Buffers of TFLite model are mostly used for constant tensors.\n",
      "  And zero value buffers are buffers filled with zeros.\n",
      "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
      "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ret_model = tf.lite.TFLiteConverter.from_keras_model(model).convert()\n",
    "\n",
    "tf.lite.experimental.Analyzer.analyze(model_content=ret_model,gpu_compatibility=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp1kju2p8l/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp1kju2p8l/assets\n",
      "/home/developer/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:887: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = represent_data_gen\n",
    "tflite_16x8_model = converter.convert()\n",
    "tflite_model_16x8_file = tflite_models_dir/\"avl_model_quant_16x8.tflite\"\n",
    "tflite_model_16x8_file.write_bytes(tflite_16x8_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.tflite816', 'wb') as f:\n",
    "  f.write(tflite_16x8_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('GHL_Models/resnet50_csv_26.h5', backbone_name='resnet50')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_models_dir = pathlib.Path(\"/tmp/avl_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145372884"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_file = tflite_models_dir/\"avl_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.io.read_file('PyAVE_cmu_fall2023/cmu_fall2023/images/validation_image_cmu/10.6.3.9 Flat margin.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_retinanet.keras_retinanet.models import load_model\n",
    "from keras_retinanet.keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "\n",
    "model = load_model('GHL_Models/resnet50_csv_26.h5', backbone_name='resnet50')\n",
    "image = read_image_bgr('PyAVE_cmu_fall2023/cmu_fall2023/images/validation_image_cmu/10.6.3.9 Flat margin.jpg')\n",
    "\n",
    "# copy to draw on\n",
    "draw = image.copy()\n",
    "draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# preprocess image for network\n",
    "image = preprocess_image(image)\n",
    "image, scale = resize_image(image)\n",
    "\n",
    "boxes, scores = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "print(boxes, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.00081564, 0.00078893],\n",
       "        [0.00068027, 0.00076989],\n",
       "        [0.00073375, 0.00084641],\n",
       "        ...,\n",
       "        [0.00160126, 0.00288163],\n",
       "        [0.0016234 , 0.00201132],\n",
       "        [0.00080883, 0.00125047]]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp3kr24sjp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp3kr24sjp/assets\n",
      "2023-11-13 17:40:33.580996: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-11-13 17:40:33.581021: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-11-13 17:40:33.581269: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp3kr24sjp\n",
      "2023-11-13 17:40:33.610458: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2023-11-13 17:40:33.610502: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmp3kr24sjp\n",
      "2023-11-13 17:40:33.707206: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-11-13 17:40:34.787665: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp3kr24sjp\n",
      "2023-11-13 17:40:35.082260: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 1501006 microseconds.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[39myield\u001b[39;00m [input_value]\n\u001b[1;32m      6\u001b[0m converter\u001b[39m.\u001b[39mrepresentative_dataset \u001b[39m=\u001b[39m representative_data_gen\n\u001b[0;32m----> 7\u001b[0m tflite_16x8_model \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39;49mconvert()\n\u001b[1;32m      8\u001b[0m tflite_model_16x8_file \u001b[39m=\u001b[39m tflite_models_dir\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mavl_model_quant_16x8.tflite\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m tflite_model_16x8_file\u001b[39m.\u001b[39mwrite_bytes(tflite_16x8_model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1065\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(convert_func)\n\u001b[1;32m   1063\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1064\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1065\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_and_export_metrics(convert_func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1042\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_conversion_params_metric()\n\u001b[1;32m   1041\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[0;32m-> 1042\u001b[0m result \u001b[39m=\u001b[39m convert_func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1043\u001b[0m elapsed_time_ms \u001b[39m=\u001b[39m (time\u001b[39m.\u001b[39mprocess_time() \u001b[39m-\u001b[39m start_time) \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m result:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1526\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[39m@_export_metrics\u001b[39m\n\u001b[1;32m   1514\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1515\u001b[0m   \u001b[39m\"\"\"Converts a keras model based on instance variables.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \n\u001b[1;32m   1517\u001b[0m \u001b[39m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[39m      Invalid quantization parameters.\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1526\u001b[0m   saved_model_convert_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_as_saved_model()\n\u001b[1;32m   1527\u001b[0m   \u001b[39mif\u001b[39;00m saved_model_convert_result:\n\u001b[1;32m   1528\u001b[0m     \u001b[39mreturn\u001b[39;00m saved_model_convert_result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1507\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1503\u001b[0m   graph_def, input_tensors, output_tensors \u001b[39m=\u001b[39m (\n\u001b[1;32m   1504\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_keras_to_saved_model(temp_dir)\n\u001b[1;32m   1505\u001b[0m   )\n\u001b[1;32m   1506\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msaved_model_dir:\n\u001b[0;32m-> 1507\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(TFLiteKerasModelConverterV2, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mconvert(\n\u001b[1;32m   1508\u001b[0m         graph_def, input_tensors, output_tensors\n\u001b[1;32m   1509\u001b[0m     )\n\u001b[1;32m   1510\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1511\u001b[0m   shutil\u001b[39m.\u001b[39mrmtree(temp_dir, \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1303\u001b[0m, in \u001b[0;36mTFLiteConverterBaseV2.convert\u001b[0;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[39m# Converts model.\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m result \u001b[39m=\u001b[39m _convert_graphdef(\n\u001b[1;32m   1297\u001b[0m     input_data\u001b[39m=\u001b[39mgraph_def,\n\u001b[1;32m   1298\u001b[0m     input_tensors\u001b[39m=\u001b[39minput_tensors,\n\u001b[1;32m   1299\u001b[0m     output_tensors\u001b[39m=\u001b[39moutput_tensors,\n\u001b[1;32m   1300\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconverter_kwargs,\n\u001b[1;32m   1301\u001b[0m )\n\u001b[0;32m-> 1303\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimize_tflite_model(\n\u001b[1;32m   1304\u001b[0m     result, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_quant_mode, quant_io\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexperimental_new_quantizer\n\u001b[1;32m   1305\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:215\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    214\u001b[0m   report_error_message(\u001b[39mstr\u001b[39m(error))\n\u001b[0;32m--> 215\u001b[0m   \u001b[39mraise\u001b[39;00m error \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m   \u001b[39mexcept\u001b[39;00m ConverterError \u001b[39mas\u001b[39;00m converter_error:\n\u001b[1;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m converter_error\u001b[39m.\u001b[39merrors:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:991\u001b[0m, in \u001b[0;36mTFLiteConverterBase._optimize_tflite_model\u001b[0;34m(self, model, quant_mode, quant_io)\u001b[0m\n\u001b[1;32m    989\u001b[0m   q_allow_float \u001b[39m=\u001b[39m quant_mode\u001b[39m.\u001b[39mis_allow_float()\n\u001b[1;32m    990\u001b[0m   q_variable_quantization \u001b[39m=\u001b[39m quant_mode\u001b[39m.\u001b[39menable_mlir_variable_quantization\n\u001b[0;32m--> 991\u001b[0m   model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_quantize(\n\u001b[1;32m    992\u001b[0m       model,\n\u001b[1;32m    993\u001b[0m       q_in_type,\n\u001b[1;32m    994\u001b[0m       q_out_type,\n\u001b[1;32m    995\u001b[0m       q_activations_type,\n\u001b[1;32m    996\u001b[0m       q_bias_type,\n\u001b[1;32m    997\u001b[0m       q_allow_float,\n\u001b[1;32m    998\u001b[0m       q_variable_quantization,\n\u001b[1;32m    999\u001b[0m   )\n\u001b[1;32m   1001\u001b[0m m_in_type \u001b[39m=\u001b[39m in_type \u001b[39mif\u001b[39;00m in_type \u001b[39melse\u001b[39;00m _dtypes\u001b[39m.\u001b[39mfloat32\n\u001b[1;32m   1002\u001b[0m m_out_type \u001b[39m=\u001b[39m out_type \u001b[39mif\u001b[39;00m out_type \u001b[39melse\u001b[39;00m _dtypes\u001b[39m.\u001b[39mfloat32\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:710\u001b[0m, in \u001b[0;36mTFLiteConverterBase._quantize\u001b[0;34m(self, result, input_type, output_type, activations_type, bias_type, allow_float, enable_variable_quantization)\u001b[0m\n\u001b[1;32m    706\u001b[0m calibrate_quantize \u001b[39m=\u001b[39m _calibrator\u001b[39m.\u001b[39mCalibrator(\n\u001b[1;32m    707\u001b[0m     result, custom_op_registerers_by_name, custom_op_registerers_by_func\n\u001b[1;32m    708\u001b[0m )\n\u001b[1;32m    709\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experimental_calibrate_only \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_new_quantizer:\n\u001b[0;32m--> 710\u001b[0m   calibrated \u001b[39m=\u001b[39m calibrate_quantize\u001b[39m.\u001b[39;49mcalibrate(\n\u001b[1;32m    711\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepresentative_dataset\u001b[39m.\u001b[39;49minput_gen\n\u001b[1;32m    712\u001b[0m   )\n\u001b[1;32m    714\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experimental_calibrate_only:\n\u001b[1;32m    715\u001b[0m   \u001b[39mreturn\u001b[39;00m calibrated\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:215\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    214\u001b[0m   report_error_message(\u001b[39mstr\u001b[39m(error))\n\u001b[0;32m--> 215\u001b[0m   \u001b[39mraise\u001b[39;00m error \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m   \u001b[39mexcept\u001b[39;00m ConverterError \u001b[39mas\u001b[39;00m converter_error:\n\u001b[1;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m converter_error\u001b[39m.\u001b[39merrors:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py:254\u001b[0m, in \u001b[0;36mCalibrator.calibrate\u001b[0;34m(self, dataset_gen)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39m@convert_phase\u001b[39m(Component\u001b[39m.\u001b[39mOPTIMIZE_TFLITE_MODEL, SubComponent\u001b[39m.\u001b[39mCALIBRATE)\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalibrate\u001b[39m(\u001b[39mself\u001b[39m, dataset_gen):\n\u001b[1;32m    246\u001b[0m   \u001b[39m\"\"\"Calibrates the model with specified generator.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[39m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39m    dataset_gen: A generator that generates calibration samples.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feed_tensors(dataset_gen, resize_input\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    255\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calibrator\u001b[39m.\u001b[39mCalibrate()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py:143\u001b[0m, in \u001b[0;36mCalibrator._feed_tensors\u001b[0;34m(self, dataset_gen, resize_input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calibrator\u001b[39m.\u001b[39mPrepare(\n\u001b[1;32m    140\u001b[0m         [\u001b[39mlist\u001b[39m(s\u001b[39m.\u001b[39mshape) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m input_array], signature_key\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calibrator\u001b[39m.\u001b[39mPrepare([\u001b[39mlist\u001b[39m(s\u001b[39m.\u001b[39mshape) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m input_array])\n\u001b[1;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m   \u001b[39mif\u001b[39;00m signature_key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py:143\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calibrator\u001b[39m.\u001b[39mPrepare(\n\u001b[1;32m    140\u001b[0m         [\u001b[39mlist\u001b[39m(s\u001b[39m.\u001b[39mshape) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m input_array], signature_key\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calibrator\u001b[39m.\u001b[39mPrepare([\u001b[39mlist\u001b[39m(s\u001b[39m.\u001b[39;49mshape) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m input_array])\n\u001b[1;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m   \u001b[39mif\u001b[39;00m signature_key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "mnist_ds = tf.data.Dataset.from_tensor_slices((tf.expand_dims(image, axis=0),)).batch(1)\n",
    "def representative_data_gen():\n",
    "    for input_value in mnist_ds.take(100):\n",
    "        # Model has only one input so each data point has one element.\n",
    "        yield [input_value]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "tflite_16x8_model = converter.convert()\n",
    "tflite_model_16x8_file = tflite_models_dir/\"avl_model_quant_16x8.tflite\"\n",
    "tflite_model_16x8_file.write_bytes(tflite_16x8_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = tf.cast(image, tf.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_dataset = tf.data.Dataset.from_tensor_slices((images)).batch(1)\n",
    "def representative_data_gen():\n",
    "    \n",
    "    yield [repr_dataset.take(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TakeDataset element_spec=TensorSpec(shape=(None, 1067, 3), dtype=tf.float32, name=None)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "converter.representative_dataset = representative_data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6xab3ws8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6xab3ws8/assets\n",
      "2023-11-13 17:28:42.176619: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-11-13 17:28:42.176645: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-11-13 17:28:42.178758: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp6xab3ws8\n",
      "2023-11-13 17:28:42.223442: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2023-11-13 17:28:42.223485: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmp6xab3ws8\n",
      "2023-11-13 17:28:42.354850: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-11-13 17:28:43.838328: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp6xab3ws8\n",
      "2023-11-13 17:28:44.173368: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 1994611 microseconds.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tflite_16x8_model \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39;49mconvert()\n\u001b[1;32m      2\u001b[0m tflite_model_16x8_file \u001b[39m=\u001b[39m tflite_models_dir\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mavl_model_quant_16x8.tflite\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m tflite_model_16x8_file\u001b[39m.\u001b[39mwrite_bytes(tflite_16x8_model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1065\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(convert_func)\n\u001b[1;32m   1063\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1064\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1065\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_and_export_metrics(convert_func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1042\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_conversion_params_metric()\n\u001b[1;32m   1041\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[0;32m-> 1042\u001b[0m result \u001b[39m=\u001b[39m convert_func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1043\u001b[0m elapsed_time_ms \u001b[39m=\u001b[39m (time\u001b[39m.\u001b[39mprocess_time() \u001b[39m-\u001b[39m start_time) \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m result:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1526\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[39m@_export_metrics\u001b[39m\n\u001b[1;32m   1514\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1515\u001b[0m   \u001b[39m\"\"\"Converts a keras model based on instance variables.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \n\u001b[1;32m   1517\u001b[0m \u001b[39m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[39m      Invalid quantization parameters.\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1526\u001b[0m   saved_model_convert_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_as_saved_model()\n\u001b[1;32m   1527\u001b[0m   \u001b[39mif\u001b[39;00m saved_model_convert_result:\n\u001b[1;32m   1528\u001b[0m     \u001b[39mreturn\u001b[39;00m saved_model_convert_result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1507\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1503\u001b[0m   graph_def, input_tensors, output_tensors \u001b[39m=\u001b[39m (\n\u001b[1;32m   1504\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_keras_to_saved_model(temp_dir)\n\u001b[1;32m   1505\u001b[0m   )\n\u001b[1;32m   1506\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msaved_model_dir:\n\u001b[0;32m-> 1507\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(TFLiteKerasModelConverterV2, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mconvert(\n\u001b[1;32m   1508\u001b[0m         graph_def, input_tensors, output_tensors\n\u001b[1;32m   1509\u001b[0m     )\n\u001b[1;32m   1510\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1511\u001b[0m   shutil\u001b[39m.\u001b[39mrmtree(temp_dir, \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1303\u001b[0m, in \u001b[0;36mTFLiteConverterBaseV2.convert\u001b[0;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[39m# Converts model.\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m result \u001b[39m=\u001b[39m _convert_graphdef(\n\u001b[1;32m   1297\u001b[0m     input_data\u001b[39m=\u001b[39mgraph_def,\n\u001b[1;32m   1298\u001b[0m     input_tensors\u001b[39m=\u001b[39minput_tensors,\n\u001b[1;32m   1299\u001b[0m     output_tensors\u001b[39m=\u001b[39moutput_tensors,\n\u001b[1;32m   1300\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconverter_kwargs,\n\u001b[1;32m   1301\u001b[0m )\n\u001b[0;32m-> 1303\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimize_tflite_model(\n\u001b[1;32m   1304\u001b[0m     result, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_quant_mode, quant_io\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexperimental_new_quantizer\n\u001b[1;32m   1305\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:215\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    214\u001b[0m   report_error_message(\u001b[39mstr\u001b[39m(error))\n\u001b[0;32m--> 215\u001b[0m   \u001b[39mraise\u001b[39;00m error \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m   \u001b[39mexcept\u001b[39;00m ConverterError \u001b[39mas\u001b[39;00m converter_error:\n\u001b[1;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m converter_error\u001b[39m.\u001b[39merrors:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:991\u001b[0m, in \u001b[0;36mTFLiteConverterBase._optimize_tflite_model\u001b[0;34m(self, model, quant_mode, quant_io)\u001b[0m\n\u001b[1;32m    989\u001b[0m   q_allow_float \u001b[39m=\u001b[39m quant_mode\u001b[39m.\u001b[39mis_allow_float()\n\u001b[1;32m    990\u001b[0m   q_variable_quantization \u001b[39m=\u001b[39m quant_mode\u001b[39m.\u001b[39menable_mlir_variable_quantization\n\u001b[0;32m--> 991\u001b[0m   model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_quantize(\n\u001b[1;32m    992\u001b[0m       model,\n\u001b[1;32m    993\u001b[0m       q_in_type,\n\u001b[1;32m    994\u001b[0m       q_out_type,\n\u001b[1;32m    995\u001b[0m       q_activations_type,\n\u001b[1;32m    996\u001b[0m       q_bias_type,\n\u001b[1;32m    997\u001b[0m       q_allow_float,\n\u001b[1;32m    998\u001b[0m       q_variable_quantization,\n\u001b[1;32m    999\u001b[0m   )\n\u001b[1;32m   1001\u001b[0m m_in_type \u001b[39m=\u001b[39m in_type \u001b[39mif\u001b[39;00m in_type \u001b[39melse\u001b[39;00m _dtypes\u001b[39m.\u001b[39mfloat32\n\u001b[1;32m   1002\u001b[0m m_out_type \u001b[39m=\u001b[39m out_type \u001b[39mif\u001b[39;00m out_type \u001b[39melse\u001b[39;00m _dtypes\u001b[39m.\u001b[39mfloat32\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:710\u001b[0m, in \u001b[0;36mTFLiteConverterBase._quantize\u001b[0;34m(self, result, input_type, output_type, activations_type, bias_type, allow_float, enable_variable_quantization)\u001b[0m\n\u001b[1;32m    706\u001b[0m calibrate_quantize \u001b[39m=\u001b[39m _calibrator\u001b[39m.\u001b[39mCalibrator(\n\u001b[1;32m    707\u001b[0m     result, custom_op_registerers_by_name, custom_op_registerers_by_func\n\u001b[1;32m    708\u001b[0m )\n\u001b[1;32m    709\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experimental_calibrate_only \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_new_quantizer:\n\u001b[0;32m--> 710\u001b[0m   calibrated \u001b[39m=\u001b[39m calibrate_quantize\u001b[39m.\u001b[39;49mcalibrate(\n\u001b[1;32m    711\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepresentative_dataset\u001b[39m.\u001b[39;49minput_gen\n\u001b[1;32m    712\u001b[0m   )\n\u001b[1;32m    714\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experimental_calibrate_only:\n\u001b[1;32m    715\u001b[0m   \u001b[39mreturn\u001b[39;00m calibrated\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:215\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    214\u001b[0m   report_error_message(\u001b[39mstr\u001b[39m(error))\n\u001b[0;32m--> 215\u001b[0m   \u001b[39mraise\u001b[39;00m error \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m   \u001b[39mexcept\u001b[39;00m ConverterError \u001b[39mas\u001b[39;00m converter_error:\n\u001b[1;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m converter_error\u001b[39m.\u001b[39merrors:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py:254\u001b[0m, in \u001b[0;36mCalibrator.calibrate\u001b[0;34m(self, dataset_gen)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39m@convert_phase\u001b[39m(Component\u001b[39m.\u001b[39mOPTIMIZE_TFLITE_MODEL, SubComponent\u001b[39m.\u001b[39mCALIBRATE)\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalibrate\u001b[39m(\u001b[39mself\u001b[39m, dataset_gen):\n\u001b[1;32m    246\u001b[0m   \u001b[39m\"\"\"Calibrates the model with specified generator.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[39m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39m    dataset_gen: A generator that generates calibration samples.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feed_tensors(dataset_gen, resize_input\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    255\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calibrator\u001b[39m.\u001b[39mCalibrate()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py:143\u001b[0m, in \u001b[0;36mCalibrator._feed_tensors\u001b[0;34m(self, dataset_gen, resize_input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calibrator\u001b[39m.\u001b[39mPrepare(\n\u001b[1;32m    140\u001b[0m         [\u001b[39mlist\u001b[39m(s\u001b[39m.\u001b[39mshape) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m input_array], signature_key\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calibrator\u001b[39m.\u001b[39mPrepare([\u001b[39mlist\u001b[39m(s\u001b[39m.\u001b[39mshape) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m input_array])\n\u001b[1;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m   \u001b[39mif\u001b[39;00m signature_key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py:143\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calibrator\u001b[39m.\u001b[39mPrepare(\n\u001b[1;32m    140\u001b[0m         [\u001b[39mlist\u001b[39m(s\u001b[39m.\u001b[39mshape) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m input_array], signature_key\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calibrator\u001b[39m.\u001b[39mPrepare([\u001b[39mlist\u001b[39m(s\u001b[39m.\u001b[39;49mshape) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m input_array])\n\u001b[1;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m   \u001b[39mif\u001b[39;00m signature_key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lh {tflite_models_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=str('model.tflite816'))\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1202, 800, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "image = preprocess_image(image)\n",
    "image, scale = resize_image(image)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1202, 800, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_image = np.expand_dims(image, axis=0).astype(np.float32)\n",
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = read_image_bgr('/home/developer/Documents/CMU/Capstone_Codes/cervical-cancer-project/images/Cats_Test0.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Dimension mismatch. Got 800 but expected 1 for dimension 1 of input 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m input_index \u001b[39m=\u001b[39m interpreter\u001b[39m.\u001b[39mget_input_details()[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m output_index \u001b[39m=\u001b[39m interpreter\u001b[39m.\u001b[39mget_output_details()[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m interpreter\u001b[39m.\u001b[39;49mset_tensor(input_index,test_image )\n\u001b[1;32m      8\u001b[0m interpreter\u001b[39m.\u001b[39minvoke()\n\u001b[1;32m      9\u001b[0m predictions \u001b[39m=\u001b[39m interpreter\u001b[39m.\u001b[39mget_tensor(output_index)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720\u001b[0m, in \u001b[0;36mInterpreter.set_tensor\u001b[0;34m(self, tensor_index, value)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_tensor\u001b[39m(\u001b[39mself\u001b[39m, tensor_index, value):\n\u001b[1;32m    705\u001b[0m   \u001b[39m\"\"\"Sets the value of the input tensor.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \n\u001b[1;32m    707\u001b[0m \u001b[39m  Note this copies data in `value`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[39m    ValueError: If the interpreter could not set the tensor.\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 720\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpreter\u001b[39m.\u001b[39;49mSetTensor(tensor_index, value)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Dimension mismatch. Got 800 but expected 1 for dimension 1 of input 0."
     ]
    }
   ],
   "source": [
    "\n",
    "image = preprocess_image(image)\n",
    "image, scale = resize_image(image)\n",
    "test_image = np.expand_dims(image, axis=0).astype(np.float32)\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "interpreter.set_tensor(input_index,test_image )\n",
    "interpreter.invoke()\n",
    "predictions = interpreter.get_tensor(output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "height = input_details[0]['shape'][1]\n",
    "width = input_details[0]['shape'][2]\n",
    "\n",
    "floating_model = (input_details[0]['dtype'] == np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_input_1:0',\n",
       "  'index': 0,\n",
       "  'shape': array([1, 1, 1, 3], dtype=int32),\n",
       "  'shape_signature': array([-1, -1, -1,  3], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
